{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558d36bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a330ac",
   "metadata": {},
   "source": [
    "## Exporting Ranking Models\n",
    "\n",
    "In this example notebook, we use the Ali-CCP: Alibaba Click and Conversion Prediction dataset to build our recommender system models. To download the training and test datasets visit Ali-CCP: Alibaba Click and Conversion Prediction at [tianchi.aliyun.com](https://tianchi.aliyun.com/dataset/dataDetail?dataId=408#1). We have curated the raw dataset via this [script]() and generated the parquet files that we will use in this example.\n",
    "\n",
    "### Learning objectives\n",
    "- Preparing the data with NVTabular\n",
    "- Training a DLRM model with Merlin Models\n",
    "- Exporting NVTabular workflow and ranking model for model deployment with Merlin Systems library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941927d5",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4cc552",
   "metadata": {},
   "source": [
    "Let's start with importing the libraries that we'll use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb27032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 19:28:58.918065: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-25 19:29:00.044340: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:214] Using CUDA malloc Async allocator for GPU: 0\n",
      "2022-03-25 19:29:00.044471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "import cudf\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "\n",
    "from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.schema import Schema\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "import merlin.models.tf.dataset as tf_dataloader\n",
    "\n",
    "from merlin.io.dataset import Dataset\n",
    "from merlin.schema.io.tensorflow_metadata import TensorflowMetadata\n",
    "from merlin.models.tf.blocks.core.aggregation import CosineSimilarity\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c848f07",
   "metadata": {},
   "source": [
    "## Feature Engineering with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc27b527",
   "metadata": {},
   "source": [
    "When we work on a new recommender systems, we explore the dataset, first. In doing so, we define our input and output paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "316627d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/workspace/data/\")\n",
    "train_path = os.path.join(DATA_FOLDER, 'train/' '*.parquet')\n",
    "valid_path = os.path.join(DATA_FOLDER, 'test/', '*.parquet')\n",
    "output_path = '/workspace/data/processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f062d80",
   "metadata": {},
   "source": [
    "We use a utility function, `workflow_fit_transform` perform to fit and transform steps on the raw dataset applying the operators defined in the NVTabular workflow pipeline below, and also save our workflow model. After fit and transform, the processed parquet files are saved to `output_path` and our NVTabular workflow model will be saved in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00005f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:1253: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.7 s, sys: 20.2 s, total: 36.9 s\n",
      "Wall time: 39.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "user_id = [\"user_id\"] >> Categorify() >> TagAsUserID()\n",
    "item_id = [\"item_id\"] >> Categorify() >> TagAsItemID()\n",
    "targets = [\"click\"] >> AddMetadata(tags=[str(Tags.BINARY_CLASSIFICATION), \"target\"])\n",
    "\n",
    "item_features = [\"item_category\", \"item_shop\", \"item_brand\"] >> Categorify() >> TagAsItemFeatures()\n",
    "\n",
    "user_features = ['user_shops', 'user_profile', 'user_group', \n",
    "       'user_gender', 'user_age', 'user_consumption_2', 'user_is_occupied',\n",
    "       'user_geography', 'user_intentions', 'user_brands', 'user_categories'] \\\n",
    "        >> Categorify() >> TagAsUserFeatures()\n",
    "\n",
    "outputs = user_id + item_id + item_features + user_features + targets\n",
    "\n",
    "workflow_fit_transform(outputs, train_path, valid_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9febed6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build and Train a DLRM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d1515",
   "metadata": {},
   "source": [
    "Deep Learning Recommendation Model [(DLRM)](https://arxiv.org/abs/1906.00091) architecture is a popular neural network model originally proposed by Facebook in 2019. To learn more about the DLRM model you can visit the [03-Exploring-different-models.ipynb](https://github.com/NVIDIA-Merlin/models/blob/main/examples/03-Exploring-different-models.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203fc21",
   "metadata": {},
   "source": [
    "NVTabular workflow exports the schema file of our processed dataset. To learn more about the schema object and schema file you can explore [02-Merlin-Models-and-NVTabular-applying-to-your-own-dataset.ipynb](https://github.com/NVIDIA-Merlin/models/blob/main/examples/02-Merlin-Models-and-NVTabular-applying-to-your-own-dataset.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "176d026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and valid dataset objects\n",
    "train = Dataset(os.path.join(output_path, 'train', '*.parquet'), part_size=\"500MB\")\n",
    "valid = Dataset(os.path.join(output_path, 'valid', '*.parquet'), part_size=\"500MB\")\n",
    "\n",
    "# define schema object\n",
    "schema = train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54fb4fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'item_id',\n",
       " 'item_category',\n",
       " 'item_shop',\n",
       " 'item_brand',\n",
       " 'user_shops',\n",
       " 'user_profile',\n",
       " 'user_group',\n",
       " 'user_gender',\n",
       " 'user_age',\n",
       " 'user_consumption_2',\n",
       " 'user_is_occupied',\n",
       " 'user_geography',\n",
       " 'user_intentions',\n",
       " 'user_brands',\n",
       " 'user_categories',\n",
       " 'click']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78fc8363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'click'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8456aeee",
   "metadata": {},
   "source": [
    "We print out all the features that are included in the `schema.pbtxt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d985d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DLRMModel(\n",
    "    schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column, metrics=[tf.keras.metrics.AUC()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed914ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 19:30:23.157631: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2442/2442 [==============================] - ETA: 0s - auc: 0.6464 - loss: 0.1615 - regularization_loss: 0.0000e+00 - total_loss: 0.1615"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-25 19:34:39.193897: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: cond/then/_0/cond/cond/branch_executed/_161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2442/2442 [==============================] - 307s 114ms/step - auc: 0.6464 - loss: 0.1615 - regularization_loss: 0.0000e+00 - total_loss: 0.1615 - val_auc: 0.6048 - val_loss: 0.1200 - val_regularization_loss: 0.0000e+00 - val_total_loss: 0.1200\n",
      "CPU times: user 8min, sys: 1min 31s, total: 9min 31s\n",
      "Wall time: 5min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff01819e970>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.compile('adam', run_eagerly=False)\n",
    "model.fit(train, validation_data=valid, batch_size=16*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841c993",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f6b9e",
   "metadata": {},
   "source": [
    "The last step of machine learning (ML)/deep learning (DL) pipeline is to deploy the ETL workflow and saved model to production. In the production setting, we want to transform the input data as done during training (ETL). We need to apply the same mean/std for continuous features and use the same categorical mapping to convert the categories to continuous integer before we use the DL model for a prediction. Therefore, we deploy the NVTabular workflow with the Tensorflow model as an ensemble model to Triton Inference using [Merlin Systems](https://github.com/NVIDIA-Merlin/systems) library very easily. The ensemble model guarantees that the same transformation is applied to the raw inputs.\n",
    "\n",
    "We save our DLRM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1033ecd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as sequential_block_10_layer_call_fn, sequential_block_10_layer_call_and_return_conditional_losses, binary_classification_task_layer_call_fn, binary_classification_task_layer_call_and_return_conditional_losses, click/binary_classification_task/output_layer_layer_call_fn while saving (showing 5 of 48). These functions will not be directly callable after loading.\n",
      "2022-03-25 19:35:38.064456: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 788046592 exceeds 10% of free system memory.\n",
      "2022-03-25 19:35:38.651699: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 788046592 exceeds 10% of free system memory.\n",
      "2022-03-25 19:35:39.240066: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 788046592 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dlrm/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dlrm/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('dlrm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f4cf0",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c933319c",
   "metadata": {},
   "source": [
    "We trained and exported our ranking model and NVTabular workflow. In the next step, we will learn how to deploy our trained DLRM model into [Triton Inference Server](https://github.com/triton-inference-server/server) with [Merlin Sytems](https://github.com/NVIDIA-Merlin/systems) library. NVIDIA Triton Inference Server (TIS) simplifies the deployment of AI models at scale in production. TIS provides a cloud and edge inferencing solution optimized for both CPUs and GPUs. It supports a number of different machine learning frameworks such as TensorFlow and PyTorch.\n",
    "\n",
    "Visit `examples/Getting_Started` folder and continue on the nex step with executing `Getting-started-with-Merlin-Systems` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
