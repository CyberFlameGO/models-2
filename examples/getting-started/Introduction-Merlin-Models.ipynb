{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d70ca427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42241917",
   "metadata": {},
   "source": [
    "## MERLIN MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613ed2b",
   "metadata": {},
   "source": [
    "Recommender Systems (RecSys) attempt to accurately predict users' preferences by supporting users in the process of finding and selecting products (items) from a given catalog, thus enhance their engagement and overall satisfaction with online services. Several traditional (e.g. collaborative filtering) and Deep Learning-based RecSys models have been built and used in industry within the last decade. Recently, deep learning based recommender systems have become the norm for large scale industry applications. In contrast to the other domains, DL recommender models aren’t able to effectively leverage the parallel compute and high memory bandwidth that GPUs have to offer. To address the challenges in preprocessing, training and deploying recommender systems at scale, NVIDIA developed an open source framework, called [Merlin](https://github.com/NVIDIA-Merlin). One of the libraries of the Merlin framework is Merlin Models that is being developed to provide all the necessary tools to design an end-to-end RecSys pipeline, offering flexibility at each stage: multiple inputs processing/representation modules, different layers for designing the model’s architecture, different prediction heads, loss functions, different negative sampling techniques etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb0b93",
   "metadata": {},
   "source": [
    "The goal of the `Merlin Models` library is make it easy for users in industry or academia to train and deploy recommender models with best practices baked into the library. This will let users in industry easily train standard models against their own dataset, getting high performance GPU accelerated models into production. This will also let researchers to build custom models by incorporating standard components of deep learning recommender models, and then benchmark their new models on example offline datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f3c5a",
   "metadata": {},
   "source": [
    "### Core features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a016301",
   "metadata": {},
   "source": [
    "- Support for diverse input types (e.g. categorical and continuous), architectures (e.g. two-tower or sequential) or tasks (e.g. binary, multi-class classification, multi-task)\n",
    "- Flexible building blocks to define a broad range of models with various prediction tasks, loss functions and negative sampling techniques \n",
    "- Flexible APIs targeted to both production and research\n",
    "- Unified API enables users to create models in TensorFlow or PyTorch\n",
    "- GPU-optimized data-loading, model training and serving\n",
    "- Deep integration with NVTabular for ETL and model serving\n",
    "- Multi-task learning is a first-class citizen\n",
    "- State-of-the-art negative sampling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c593b862",
   "metadata": {},
   "source": [
    "### Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80c1cdf",
   "metadata": {},
   "source": [
    "In this introductory notebook we aim at\n",
    "\n",
    "- introducing the building blocks of Merlin Models library\n",
    "- showing how seamlessly integrate NVTabular and Merlin Models libraries\n",
    "- training DL-based recommender models with optimized Tensorflow data loaders with only a few lines of code for `BinaryClassification` task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970e11b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229db2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/workspace/merlin_models/\")\n",
    "sys.path.append(\"/nvtabular/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bafb3cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 13:33:57.550567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import nvtabular as nvt\n",
    "import merlin_models.tf as ml\n",
    "from merlin_standard_lib import Schema, Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c55e8",
   "metadata": {},
   "source": [
    "## ETL with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d344cbf0",
   "metadata": {},
   "source": [
    "We use the [MovieLens25M](https://grouplens.org/datasets/movielens/25m/) dataset which is a popular dataset for recommender systems and is used in academic publications. We developed a utility functions for data downloading, converting and preprocessing. For data preprocessing and feature engineering, we are using [NVTabular]((https://github.com/NVIDIA-Merlin/NVTabular) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "469c6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ff3770",
   "metadata": {},
   "source": [
    "Avoid Numba low occupancy warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7acfc5",
   "metadata": {},
   "source": [
    "We define our base input directory, containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c3688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get(\n",
    "    \"INPUT_DATA_DIR\", os.path.expanduser(\"/workspace/data/movielens/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc4ba94",
   "metadata": {},
   "source": [
    "With help of a utility function first we download and unzip the data. Second, we convert data via basic preprocessing, and split data into train and validation files and save them as parquet files. Afterwards, we preprocess the train and validation parquet files and generate features for model training using NVTabular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc0128b",
   "metadata": {},
   "source": [
    "Let's download Movielens 25M dataset and then process it, and save files to disk in parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9370279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin_standard_lib.utils.data_etl_utils import movielens_download_etl\n",
    "movielens_download_etl(INPUT_DATA_DIR, 'ml-25m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54701e9",
   "metadata": {},
   "source": [
    "You can visit `01-Download-Convert.ipynb` and `02-ETL-with-NVTabular.ipynb` in `ETL` folder to see pipeline coded in jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ae933a",
   "metadata": {},
   "source": [
    "### Schema Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81331e18",
   "metadata": {},
   "source": [
    "Merlin Models library relies on a schema object that takes the input features as input and automatically builds all necessary layers to represent, normalize and aggregate input features. As you can see below, `schema.pbtxt` is a protobuf text file contains features metadata, including statistics about features such as cardinality, min and max values and also tags based on their characteristics and dtypes (e.g., categorical, continuous, list, item_id). We can tag our target column and even add the prediction task such as binary, regression or multiclass as tags for the target column in the `schema.pbtxt` file. The Schema provides a standard representation for metadata that is useful when training machine learning or deep learning models.\n",
    "\n",
    "The metadata information loaded from Schema and their tags are used to automatically set the parameters of Merlin models. Certain modules have a `from_schema()` method to instantiate their parameters and layers from protobuf text file respectively.\n",
    "\n",
    "We have already generated our `schema.pbtxt` file in the previous notebook using `NVTabular`. Now we start with reading this schema file to create a schema object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d2a3655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"movieId\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"movieId\"\n",
      "    min: 0\n",
      "    max: 56620\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"categorical\"\n",
      "    tag: \"item\"\n",
      "    tag: \"item_id\"\n",
      "    extra_metadata {\n",
      "      type_url: \"type.googleapis.com/google.protobuf.Struct\"\n",
      "      value: \"\\n\\021\\n\\013num_buckets\\022\\002\\010\\000\\n\\033\\n\\016freq_threshold\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n\\025\\n\\010max_size\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n\\030\\n\\013start_index\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n2\\n\\010cat_path\\022&\\032$.//categories/unique.movieId.parquet\\nG\\n\\017embedding_sizes\\0224*2\\n\\030\\n\\013cardinality\\022\\t\\021\\000\\000\\000\\000\\200\\245\\353@\\n\\026\\n\\tdimension\\022\\t\\021\\000\\000\\000\\000\\000\\000\\200@\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"userId\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"userId\"\n",
      "    min: 0\n",
      "    max: 162542\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"user\"\n"
     ]
    }
   ],
   "source": [
    "from merlin_standard_lib import Schema\n",
    "SCHEMA_PATH = os.path.join(INPUT_DATA_DIR, 'ml-25m' \"/train/schema.pbtxt\")\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)\n",
    "!head -30 $SCHEMA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b463af",
   "metadata": {},
   "source": [
    "## Building an MLP with Merlin Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd49f69",
   "metadata": {},
   "source": [
    "Let's start with building an MLP model to train a binary classification task using Merlin Models library. As a starting point, we are going to use only `['movieId', 'userId', 'genres']` input features. Note that our target column is `rating_binary` created in the `02-ETL-with-NVTabular` notebook as a binary target column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19c620",
   "metadata": {},
   "source": [
    "We can easily remove the features we do not want to use as input to the model with `remove_by_name` property of the schema object, and create a new schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d33fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema.remove_by_name(['rating', 'title', 'TE_movieId_rating', 'userId_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89595554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId', 'userId', 'genres', 'rating_binary']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ecbf5f",
   "metadata": {},
   "source": [
    "Below we build a TF MLP model using three main blocks:\n",
    "- [InputBlock](https://github.com/NVIDIA-Merlin/models/blob/main/merlin_models/tf/block/inputs.py) is the entry block of the model that accepts `schema` object and other arguments like `aggregation`, `continuous projection`, `embedding options`. This function creates continuous and embedding layers, and connects them via [ParallelBlock](https://github.com/NVIDIA-Merlin/models/blob/main/merlin_models/tf/core.py#L1184). If `aggregation` argument is not set, it returns a dictionary of multiple tensor each corresponds to an input feature, otherwise it merges the tensors into one using the aggregation method.\n",
    "- [ParallelBlock](https://github.com/NVIDIA-Merlin/models/blob/main/merlin_models/tf/core.py#L1184) take a list of layers as input, stacks them in parallel, and then  outputs a dictionary of tensors.\n",
    "- [MLPBlock](https://github.com/NVIDIA-Merlin/models/blob/main/merlin_models/tf/block/mlp.py) is to define a Multi-layer perceptron block, where each dimension is used to create a fully connected Dense layer. In addition to `dimension` argument we can also feed `activation`, `use_bais`, `dropout`, etc. arguments to the `MLPBlock` function.\n",
    "- [BinaryClassificationTask](https://github.com/NVIDIA-Merlin/models/blob/main/merlin_models/tf/prediction/classification.py#L30) supports the binary prediction task. Merlin Models library also supports other predictions tasks, like next-item prediction and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4e934dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default emb_dim is 64\n",
    "model = ml.Model(\n",
    "    ml.InputBlock(schema), \n",
    "    ml.MLPBlock([64, 32]),\n",
    "    ml.BinaryClassificationTask(\"rating_binary\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55c9318",
   "metadata": {},
   "source": [
    "As you can notice we start with `InputBlock`, then create an `MLP block`, and then finally, we connect our `MLP block` to the BinaryClassificationTask head to be able to do binary classification, and create our model class. Basically, this is similar to a linear stacking of layers into a tf.keras.Model as done with tf.keras.Sequential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f73465",
   "metadata": {},
   "source": [
    "### Define Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf527e",
   "metadata": {},
   "source": [
    "We're ready to start training. We'll use the NVTabular `KerasSequenceLoader` for reading chunks of parquet files. `KerasSequenceLoader` manages shuffling by loading in chunks of data from different parts of the full dataset, concatenating them and then shuffling, then iterating through this super-chunk sequentially in batches. The number of \"parts\" of the dataset that get sample, or \"partitions\", is controlled by the `parts_per_chunk` kwarg, while the size of each one of these parts is controlled by the `buffer_size` kwarg, which refers to a fraction of available GPU memory (you can read more about it [here](https://nvidia-merlin.github.io/NVTabular/main/training/tensorflow.html) and [here](https://nvidia-merlin.github.io/NVTabular/main/api/tensorflow_dataloader.html?highlight=kerassequence#nvtabular.loader.tensorflow.KerasSequenceLoader)). Using more chunks leads to better randomness, especially at the epoch level where physically disparate samples can be brought into the same batch, but can impact throughput if you use too many. In any case, the speed of the parquet reader makes feasible buffer sizes much larger.\n",
    "\n",
    "Note that `genres` column is a multi-hot column and it is fed to dataloader as a sparse tensor and then it is converted to dense represantation. Based on our analysis, genres column has max 10 sequence of entries. So we will set the sequence length for the multi-hot columns as 10 in the `sparse_feature_max` dictionary below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a7c47d",
   "metadata": {},
   "source": [
    "Note that we do not have continuous columns so we only feed categorical columns to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17b42ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import merlin_models.tf.dataset as tf_dataloader\n",
    "\n",
    "# Define categorical and continuous columns\n",
    "x_cat_names, x_cont_names = ['userId', 'movieId', 'genres'], []\n",
    "\n",
    "# dictionary representing max sequence length for each column\n",
    "sparse_features_max = {'genres': 10}\n",
    "\n",
    "def get_dataloader(paths_or_dataset, batch_size=4096, shuffle=True):\n",
    "    dataloader = tf_dataloader.Dataset(\n",
    "        paths_or_dataset,\n",
    "        batch_size=batch_size,\n",
    "        label_names=['rating_binary'],\n",
    "        cat_names=x_cat_names,\n",
    "        cont_names=x_cont_names,\n",
    "        sparse_names=list(sparse_features_max.keys()),\n",
    "        sparse_max=sparse_features_max,\n",
    "        sparse_as_dense=True,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return dataloader.map(lambda X, y: (X, tf.reshape(y, (-1,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f101b17",
   "metadata": {},
   "source": [
    "Define the parquet file paths for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b74d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"/workspace/data/movielens/ml-25m\")\n",
    "train_paths = glob.glob(os.path.join(OUTPUT_DIR, \"train/*.parquet\"))\n",
    "eval_paths = glob.glob(os.path.join(OUTPUT_DIR, \"valid/*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "266d3b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model.compile(optimizer=\"adam\", run_eagerly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f37024",
   "metadata": {},
   "source": [
    "In order to train and evaluate our model we are using `.fit()` and `.evaluate()` methods as done in `tf.keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f58c290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Launch training\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 13:34:32.959523: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-01-31 13:34:33.643624: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Block.parse of <class 'merlin_models.tf.core.Block'>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: annotated name 'output' can't be nonlocal (tmptsovfy8l.py, line 36)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "4883/4883 [==============================] - 93s 11ms/step - rating_binary/binary_classification_task/precision: 0.7612 - rating_binary/binary_classification_task/recall: 0.8615 - rating_binary/binary_classification_task/binary_accuracy: 0.7445 - rating_binary/binary_classification_task/auc: 0.8016 - loss: 0.5144 - regularization_loss: 0.0000e+00 - total_loss: 0.5144\n",
      "********************\n",
      "\n",
      "Start evaluation\n",
      "********************\n",
      "\n",
      "1221/1221 [==============================] - 19s 8ms/step - rating_binary/binary_classification_task/precision: 0.7741 - rating_binary/binary_classification_task/recall: 0.8641 - rating_binary/binary_classification_task/binary_accuracy: 0.7573 - rating_binary/binary_classification_task/auc: 0.8196 - loss: 0.4945 - regularization_loss: 0.0000e+00 - total_loss: 0.4945\n"
     ]
    }
   ],
   "source": [
    "print('*'*20)\n",
    "print(\"Launch training\")\n",
    "print('*'*20 + '\\n')\n",
    "train_loader = get_dataloader(nvt.Dataset(train_paths), shuffle=True) \n",
    "losses = model.fit(train_loader, epochs=1)\n",
    "model.reset_metrics()\n",
    "\n",
    "# Evaluate\n",
    "print('*'*20 + '\\n')\n",
    "print(\"Start evaluation\")\n",
    "print('*'*20 + '\\n')\n",
    "eval_loader = get_dataloader(nvt.Dataset(eval_paths), shuffle=False) \n",
    "eval_metrics = model.evaluate(eval_loader, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c37ee3",
   "metadata": {},
   "source": [
    "#### Connect Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f7a165",
   "metadata": {},
   "source": [
    "Now we will build the same MLP model using `connect` method. Connect method cares about the sequential order, first argument is run first. The `connect` method is works as [SequentialBlock]() class which enables users to represent a sequence of Keras layers. It is a Keras Layer that can be used instead of `tf.keras.layers.Sequential` which is actually a Keras Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6500ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ml.InputBlock(schema).connect(ml.MLPBlock([64, 32]), ml.BinaryClassificationTask(\"rating_binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "832d375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", run_eagerly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "174bb565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Launch training\n",
      "********************\n",
      "\n",
      "4883/4883 [==============================] - 93s 12ms/step - rating_binary/binary_classification_task/precision: 0.7616 - rating_binary/binary_classification_task/recall: 0.8610 - rating_binary/binary_classification_task/binary_accuracy: 0.7446 - rating_binary/binary_classification_task/auc: 0.8017 - loss: 0.5143 - regularization_loss: 0.0000e+00 - total_loss: 0.5143\n",
      "********************\n",
      "\n",
      "Start evaluation\n",
      "********************\n",
      "\n",
      "1221/1221 [==============================] - 18s 7ms/step - rating_binary/binary_classification_task/precision: 0.7731 - rating_binary/binary_classification_task/recall: 0.8666 - rating_binary/binary_classification_task/binary_accuracy: 0.7575 - rating_binary/binary_classification_task/auc: 0.8200 - loss: 0.4942 - regularization_loss: 0.0000e+00 - total_loss: 0.4942\n"
     ]
    }
   ],
   "source": [
    "print('*'*20)\n",
    "print(\"Launch training\")\n",
    "print('*'*20 + '\\n')\n",
    "train_loader = get_dataloader(nvt.Dataset(train_paths), shuffle=True) \n",
    "losses = model.fit(train_loader, epochs=1)\n",
    "model.reset_metrics()\n",
    "\n",
    "# Evaluate\n",
    "print('*'*20 + '\\n')\n",
    "print(\"Start evaluation\")\n",
    "print('*'*20 + '\\n')\n",
    "eval_loader = get_dataloader(nvt.Dataset(eval_paths), shuffle=False) \n",
    "eval_metrics = model.evaluate(eval_loader, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532ba586",
   "metadata": {},
   "source": [
    "Before moving to the next section we need to restart our kernel so that we can free up the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "969603b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aac535",
   "metadata": {},
   "source": [
    "You can restart manually in case the restart is not sucessful as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c68a89",
   "metadata": {},
   "source": [
    "### ParallelBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acffaf95",
   "metadata": {},
   "source": [
    "Now, we will add continuous features and create a new model using `ParallelBlock` class. We will create one block (like a branch) for continuous features and another one for categorical features and we will connect them. Note that this architecture is different than the ones above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8357072c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 20:19:24.916671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import merlin_models.tf as ml\n",
    "from merlin_standard_lib import Schema, Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81d89f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_PATH = os.path.join(INPUT_DATA_DIR, \"ml-25m\", \"/train/schema.pbtxt\")\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4fb93aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema.remove_by_name(['rating', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a1668de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId',\n",
       " 'userId',\n",
       " 'genres',\n",
       " 'TE_movieId_rating',\n",
       " 'userId_count',\n",
       " 'rating_binary']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36967b62",
   "metadata": {},
   "source": [
    "Note that  `TE_movieId_rating`,  and  `userId_count` are the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f295f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_schema = schema.select_by_tag(Tag.CONTINUOUS)\n",
    "cat_schema = schema.select_by_tag(Tag.CATEGORICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c01c99be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TE_movieId_rating', 'userId_count']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_schema.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d490c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId', 'userId', 'genres']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_schema.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be6c4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_block = ml.InputBlock(con_schema).connect(ml.MLPBlock([256, 64]))\n",
    "cat_block = ml.InputBlock(cat_schema).connect(ml.MLPBlock([128, 64]))\n",
    "\n",
    "body = ml.ParallelBlock({'continuous_block': cont_block, 'categorical_block': cat_block}, aggregation='concat')\n",
    "\n",
    "model = body.connect(ml.BinaryClassificationTask(\"rating_binary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a819b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"/workspace/data/movielens/ml-25m\")\n",
    "train_paths = glob.glob(os.path.join(OUTPUT_DIR, \"train/*.parquet\"))\n",
    "eval_paths = glob.glob(os.path.join(OUTPUT_DIR, \"valid/*.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7c73e",
   "metadata": {},
   "source": [
    "Notice below that this time our `x_cont_names` list is not empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "112085c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import merlin_models.tf.dataset as tf_dataloader\n",
    "\n",
    "# Define categorical and continuous columns\n",
    "x_cat_names, x_cont_names = ['userId', 'movieId', 'genres'], ['TE_movieId_rating', 'userId_count']\n",
    "\n",
    "# dictionary representing max sequence length for each column\n",
    "sparse_features_max = {'genres': 10}\n",
    "\n",
    "def get_dataloader(paths_or_dataset, batch_size=4096, shuffle=True):\n",
    "    dataloader = tf_dataloader.Dataset(\n",
    "        paths_or_dataset,\n",
    "        batch_size=batch_size,\n",
    "        label_names=['rating_binary'],\n",
    "        cat_names=x_cat_names,\n",
    "        cont_names=x_cont_names,\n",
    "        sparse_names=list(sparse_features_max.keys()),\n",
    "        sparse_max=sparse_features_max,\n",
    "        sparse_as_dense=True,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return dataloader.map(lambda X, y: (X, tf.reshape(y, (-1,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42c2b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model.compile(optimizer=\"adam\", run_eagerly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9392566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Launch training\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 20:30:47.442673: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-01-21 20:30:49.952537: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Block.parse of <class 'merlin_models.tf.core.Block'>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: annotated name 'output' can't be nonlocal (tmp3bgq5a9q.py, line 36)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "4883/4883 [==============================] - 198s 12ms/step - rating_binary/binary_classification_task/precision: 0.7627 - rating_binary/binary_classification_task/recall: 0.8617 - rating_binary/binary_classification_task/binary_accuracy: 0.7459 - rating_binary/binary_classification_task/auc: 0.8029 - loss: 0.5132 - regularization_loss: 0.0000e+00 - total_loss: 0.5132\n",
      "********************\n",
      "\n",
      "Start evaluation\n",
      "********************\n",
      "\n",
      "1221/1221 [==============================] - 38s 8ms/step - rating_binary/binary_classification_task/precision: 0.7757 - rating_binary/binary_classification_task/recall: 0.8604 - rating_binary/binary_classification_task/binary_accuracy: 0.7572 - rating_binary/binary_classification_task/auc: 0.8201 - loss: 0.4942 - regularization_loss: 0.0000e+00 - total_loss: 0.4942\n"
     ]
    }
   ],
   "source": [
    "print('*'*20)\n",
    "print(\"Launch training\")\n",
    "print('*'*20 + '\\n')\n",
    "train_loader = get_dataloader(nvt.Dataset(train_paths), shuffle=True) \n",
    "losses = model.fit(train_loader, epochs=1)\n",
    "model.reset_metrics()\n",
    "\n",
    "# Evaluate\n",
    "print('*'*20 + '\\n')\n",
    "print(\"Start evaluation\")\n",
    "print('*'*20 + '\\n')\n",
    "eval_loader = get_dataloader(nvt.Dataset(eval_paths), shuffle=False) \n",
    "eval_metrics = model.evaluate(eval_loader, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb7242",
   "metadata": {},
   "source": [
    "Like that we can easily build `BinaryClasificationTask` models with Merlin Models library in couple of lines. For more advanced ranking examples, please visit `ranking` folder and explore the other example notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
