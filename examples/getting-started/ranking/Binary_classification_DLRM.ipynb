{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0bba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c69ff",
   "metadata": {},
   "source": [
    "## Training a DLRM model with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00b4fc",
   "metadata": {},
   "source": [
    "In the previous notebooks, we have downloaded the movielens data, converted it to parquet files and then used NVTabular library to process the data, join data frames, and create input features. In this notebook we will use NVIDIA Merlin Models library to build and train a Deep Learning Recommendation Model [(DLRM)](https://arxiv.org/abs/1906.00091) architecture originally proposed by Facebook in 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a86c2",
   "metadata": {},
   "source": [
    "Figure 1 illustrates DLRM architecture. The model was introduced as a personalization deep learning model that uses embeddings to process sparse features that represent categorical data and a multilayer perceptron (MLP) to process dense features, then interacts these features explicitly using the statistical techniques proposed in [here](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5694074)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b6d0d",
   "metadata": {},
   "source": [
    "![DLRM](../images/DLRM.png)\n",
    "\n",
    "<p>Figure 2.DLRM architecture. Image source: <a href=\"https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Recommendation/DLRM\">Nvidia DL Examples</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674555df",
   "metadata": {},
   "source": [
    "DLRM accepts two types of features: categorical and numerical. \n",
    "- For each categorical feature, an embedding table is used to provide dense representation to each unique value. \n",
    "- For numerical features, they are fed to model as dense features, and then transformed by a simple neural network referred to as \"bottom MLP\". This part of the network consists of a series of linear layers with ReLU activations. \n",
    "- The output of the bottom MLP and the embedding vectors are then fed into the `dot product interaction` operation (see Pairwise interaction step). The output of \"dot interaction\" is then concatenated with the features resulting from the bottom MLP (we apply a skip-connection there) and fed into the \"top MLP\" which is also a series of dense layers with activations ((a fully connected NN). \n",
    "- The model outputs a single number (here we use sigmoid function to generate probabilities) which can be interpreted as a likelihood of a certain user clicking on an ad, watching a movie, or viewing a news page. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c28596",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6545e077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 21:42:04.377035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import nvtabular\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nvtabular as nvt\n",
    "from nvtabular.loader.tensorflow import KerasSequenceLoader\n",
    "\n",
    "import merlin_models.tf as ml\n",
    "from merlin_standard_lib import Schema, Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fda64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "logging.disable(logging.WARNING) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f268cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid Numba low occupancy warnings\n",
    "from numba import config\n",
    "config.CUDA_LOW_OCCUPANCY_WARNINGS = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e72af",
   "metadata": {},
   "source": [
    "Merlin Models library relies on a `schema` object to automatically build all necessary layers to represent, normalize and aggregate input features. As you can see below, schema.pb is a protobuf file that contains metadata including statistics about features such as cardinality, min and max values and also tags features based on their characteristics and dtypes (e.g., categorical, continuous, list, integer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2c93e",
   "metadata": {},
   "source": [
    "We have already generated our `schema.pbtxt` file in the previous notebook using NVTabular. Not we read this schema file to create a `schema` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f79c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"movieId\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"movieId\"\n",
      "    min: 0\n",
      "    max: 56690\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"item\"\n",
      "    tag: \"categorical\"\n",
      "    tag: \"item_id\"\n",
      "    extra_metadata {\n",
      "      type_url: \"type.googleapis.com/google.protobuf.Struct\"\n",
      "      value: \"\\n\\021\\n\\013num_buckets\\022\\002\\010\\000\\n\\033\\n\\016freq_threshold\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n\\025\\n\\010max_size\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n\\030\\n\\013start_index\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n2\\n\\010cat_path\\022&\\032$.//categories/unique.movieId.parquet\\nG\\n\\017embedding_sizes\\0224*2\\n\\030\\n\\013cardinality\\022\\t\\021\\000\\000\\000\\000@\\256\\353@\\n\\026\\n\\tdimension\\022\\t\\021\\000\\000\\000\\000\\000\\000\\200@\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"userId\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"userId\"\n",
      "    min: 0\n",
      "    max: 162542\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"user\"\n"
     ]
    }
   ],
   "source": [
    "from merlin_standard_lib import Schema\n",
    "SCHEMA_PATH = \"/workspace/data/movielens/train/schema.pbtxt\"\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)\n",
    "!head -30 $SCHEMA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1784d49",
   "metadata": {},
   "source": [
    "Let's remove the original `rating` and 'title' columns from the schema because we do not want to feed them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af7ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema.remove_by_name(['rating', 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee3b3f",
   "metadata": {},
   "source": [
    "We can print out the feature names including the binary target column, `rating_b`, in the schema easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de74bee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId',\n",
       " 'userId',\n",
       " 'genres',\n",
       " 'rating_b',\n",
       " 'TE_movieId_rating',\n",
       " 'userId_count']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b890bba",
   "metadata": {},
   "source": [
    "## Define the Input module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0266fe29",
   "metadata": {},
   "source": [
    "Below we define our input block using the `ml.ContinuousEmbedding` function. The from_schema() method processes the schema and creates the necessary layers to represent features and aggregate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cfb8be",
   "metadata": {},
   "source": [
    "In the next cell, the whole model is build with a few lines of code. Here is a brief explanation of the main classes and functions:\n",
    "\n",
    "- [DotProductInteraction](https://github.com/NVIDIA-Merlin/models/blob/main/merlin_models/tf/layers/interaction.py#L22) class implements the factorization machine style feature interaction layer suggested by the DLRM and DeepFM architectures. Here we do not feed an interaction type, and the `None` interaction type defaults to the standard factorization machine style interaction.\n",
    "- [TabularBlock](https://github.com/NVIDIA-Merlin/models/blob/main/merlin_models/tf/core.py) is a sub-class of `Block` class that accepts dictionary of tensors as inputs and supports the integration of many commonly used operations. This class has additional methods to apply transformations and aggregations to inputs for pre and post processing.\n",
    "- [ParallelBlock](https://github.com/NVIDIA-Merlin/models/blob/main/merlin_models/tf/core.py) class merges multiple layers or TabularBlock's into a single output of TabularData which is a dictionary of tensors. In this example, this class outputs two parallel layers of continuous and categorical blocks.\n",
    "- [BinaryClassificationTask](https://github.com/NVIDIA-Merlin/models/blob/main/merlin_models/tf/prediction/classification.py#L30) supports the binary prediction task. We also support other predictions tasks, like next-item prediction and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a114ee",
   "metadata": {},
   "source": [
    "Select continuous and categorical columns from schema using feature tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4463f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_schema = schema.select_by_tag(Tag.CONTINUOUS)\n",
    "cat_schema = schema.select_by_tag(Tag.CATEGORICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d1cb04",
   "metadata": {},
   "source": [
    "In the DLRM architecture, categorical features are processed using embeddings. Below, for each categorical feature, we create an embedding table used to provide dense representation to each unique value of this feature. The dense vector values in the embedding tables are learned during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07737e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "\n",
    "embeddings = ml.EmbeddingFeatures.from_schema(\n",
    "    cat_schema, options=ml.EmbeddingOptions(embedding_dim_default=embedding_dim)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246001da",
   "metadata": {},
   "source": [
    "We use `ContinuousFeatures` layer to build the dense layer for the continuous features and then we fed it to the MLP layer (bottom block) with `connect` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf7a9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_block = ml.MLPBlock([128, 64])\n",
    "bottom_block = ml.ContinuousFeatures.from_schema(con_schema).connect(bottom_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a93c674",
   "metadata": {},
   "source": [
    "`ParallelBlock` class outputs two parallel layers of continuous and categorical blocks, so that we can perform the dot production easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17addd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_inputs = ml.ParallelBlock({\"embeddings\": embeddings, \"bottom_block\": bottom_block})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1700770e",
   "metadata": {},
   "source": [
    "Below, we create the `dot product interaction` by taking the dot product of the bottom mlp layer output and embedding layer created from categorical features. Then we do `skip-connection` process by concatenating the bottom MLP results with the interaction layer results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f62ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DotProductInteractionBlock():\n",
    "    return ml.SequentialBlock(ml.DotProductInteraction(), pre_aggregation=\"stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58dc90b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_block_inputs = interaction_inputs.connect_with_shortcut(\n",
    "    DotProductInteractionBlock(), shortcut_filter=ml.Filter(\"continuous\"), aggregation=\"concat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c1f360",
   "metadata": {},
   "source": [
    "We then create the top MLP block and feed our concatenated features to the top block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2866bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_block = ml.MLPBlock([128, 32])\n",
    "top_block_outputs = top_block_inputs.connect(top_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11482e9c",
   "metadata": {},
   "source": [
    "Finally, we connect our top block to the BinaryClassificationTask head to be able to do binary classification, and create our `model` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e6621e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = top_block_outputs.connect(ml.BinaryClassificationTask(\"rating_b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39bfbfd",
   "metadata": {},
   "source": [
    "In addition to this low-level api code, we also have high-level api where you can define a DLRM model with only one line of code as follow:\n",
    "    \n",
    "```\n",
    "ml.DLRMBlock(schema, bottom_block=ml.MLPBlock([128, 64]), top_block=ml.MLPBlock([128, 64])\n",
    "            ).connect(ml.BinaryClassificationTask(\"rating_b\"))\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c5f7f7",
   "metadata": {},
   "source": [
    "### Define Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d5dba",
   "metadata": {},
   "source": [
    "We're ready to start training. We'll use the NVTabular `KerasSequenceLoader` for reading chunks of parquet files. `KerasSequenceLoader` manages shuffling by loading in chunks of data from different parts of the full dataset, concatenating them and then shuffling, then iterating through this super-chunk sequentially in batches. The number of \"parts\" of the dataset that get sample, or \"partitions\", is controlled by the `parts_per_chunk` kwarg, while the size of each one of these parts is controlled by the `buffer_size` kwarg, which refers to a fraction of available GPU memory (you can read more about it [here](https://nvidia-merlin.github.io/NVTabular/main/training/tensorflow.html) and [here](https://nvidia-merlin.github.io/NVTabular/main/api/tensorflow_dataloader.html?highlight=kerassequence#nvtabular.loader.tensorflow.KerasSequenceLoader)). Using more chunks leads to better randomness, especially at the epoch level where physically disparate samples can be brought into the same batch, but can impact throughput if you use too many. In any case, the speed of the parquet reader makes feasible buffer sizes much larger.\n",
    "\n",
    "Note that `genres` column is a multi-hot column and it is fed to dataloader as a sparse tensor and then it is converted to dense represantation. Based on our analysis, genres column has max 10 sequence of entries. So we will set the sequence length for the multi-hot columns as 10 in the `sparse_feature_max` dictionary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "046b108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and continuous columns\n",
    "x_cat_names, x_cont_names = ['userId', 'movieId', 'genres'], ['TE_movieId_rating','userId_count']\n",
    "\n",
    "# dictionary representing max sequence length for each column\n",
    "sparse_features_max = {'genres': 10}\n",
    "\n",
    "def get_dataloader(paths_or_dataset, batch_size=4096):\n",
    "    dataloader = KerasSequenceLoader(\n",
    "        paths_or_dataset,\n",
    "        batch_size=batch_size,\n",
    "        label_names=['rating_b'],\n",
    "        cat_names=x_cat_names,\n",
    "        cont_names=x_cont_names,\n",
    "        sparse_names=list(sparse_features_max.keys()),\n",
    "        sparse_max=sparse_features_max,\n",
    "        sparse_as_dense=True,\n",
    "    )\n",
    "    return dataloader.map(lambda X, y: (X, tf.reshape(y, (-1,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66860772",
   "metadata": {},
   "source": [
    "### Start Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79452dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"/workspace/data/movielens/\")\n",
    "train_paths = glob.glob(os.path.join(OUTPUT_DIR, \"train/*.parquet\"))\n",
    "eval_paths = glob.glob(os.path.join(OUTPUT_DIR, \"valid/*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65089250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model.compile(optimizer=\"adam\", run_eagerly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "891291ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Launch training\n",
      "********************\n",
      "\n",
      "WARNING: AutoGraph could not transform <bound method Block.parse of <class 'merlin_models.tf.core.Block'>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: annotated name 'output' can't be nonlocal (tmpv3h2awnj.py, line 36)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 20:03:55.020480: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4883/4883 [==============================] - 182s 35ms/step - rating_b/binary_classification_task/precision: 0.7618 - rating_b/binary_classification_task/recall: 0.8609 - rating_b/binary_classification_task/binary_accuracy: 0.7448 - rating_b/binary_classification_task/auc: 0.8005 - loss: 0.5160 - regularization_loss: 0.0000e+00 - total_loss: 0.516016s - rating_b/binary_classification_task/precision: 0.7607 - rating_b/binary_classification_task/recall: 0.8607\n",
      "Epoch 2/3\n",
      "4883/4883 [==============================] - 176s 35ms/step - rating_b/binary_classification_task/precision: 0.7788 - rating_b/binary_classification_task/recall: 0.8628 - rating_b/binary_classification_task/binary_accuracy: 0.7610 - rating_b/binary_classification_task/auc: 0.8255 - loss: 0.4877 - regularization_loss: 0.0000e+00 - total_loss: 0.4877\n",
      "Epoch 3/3\n",
      "4883/4883 [==============================] - 176s 35ms/step - rating_b/binary_classification_task/precision: 0.7878 - rating_b/binary_classification_task/recall: 0.8651 - rating_b/binary_classification_task/binary_accuracy: 0.7700 - rating_b/binary_classification_task/auc: 0.8379 - loss: 0.4725 - regularization_loss: 0.0000e+00 - total_loss: 0.4725\n",
      "********************\n",
      "Start evaluation\n",
      "1221/1221 [==============================] - 46s 27ms/step - rating_b/binary_classification_task/precision: 0.7810 - rating_b/binary_classification_task/recall: 0.8601 - rating_b/binary_classification_task/binary_accuracy: 0.7617 - rating_b/binary_classification_task/auc: 0.8250 - loss: 0.4902 - regularization_loss: 0.0000e+00 - total_loss: 0.4902\n",
      "********************\n",
      "\n",
      "Eval results\n",
      "\n",
      "********************\n",
      "\n",
      " loss = 0.49038803577423096\n",
      " rating_b/binary_classification_task/auc = 0.8250362873077393\n",
      " rating_b/binary_classification_task/binary_accuracy = 0.7617262601852417\n",
      " rating_b/binary_classification_task/precision = 0.7810465693473816\n",
      " rating_b/binary_classification_task/recall = 0.860145628452301\n",
      " regularization_loss = 0\n",
      " total_loss = 0.49038803577423096\n"
     ]
    }
   ],
   "source": [
    "print('*'*20)\n",
    "print(\"Launch training\")\n",
    "print('*'*20 + '\\n')\n",
    "train_loader = get_dataloader(train_paths) \n",
    "losses = model.fit(train_loader, epochs=3)\n",
    "model.reset_metrics()\n",
    "\n",
    "# Evaluate\n",
    "print('*'*20)\n",
    "print(\"Start evaluation\")\n",
    "eval_loader = get_dataloader(eval_paths) \n",
    "eval_metrics = model.evaluate(eval_loader, return_dict=True)\n",
    "\n",
    "print('*'*20 + '\\n')\n",
    "print(\"Eval results\")\n",
    "print('\\n' + '*'*20 + '\\n')\n",
    "for key in sorted(eval_metrics.keys()):\n",
    "    print(\" %s = %s\" % (key, str(eval_metrics[key]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987db53f",
   "metadata": {},
   "source": [
    "### Perform Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e810160",
   "metadata": {},
   "source": [
    "Let's use validation set and perform prediction for a given user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ca5904b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_parquet(\"/workspace/data/movielens/valid/part_0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f865d13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>userId</th>\n",
       "      <th>genres</th>\n",
       "      <th>rating_b</th>\n",
       "      <th>TE_movieId_rating</th>\n",
       "      <th>userId_count</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>15488</td>\n",
       "      <td>[1, 6]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.845424</td>\n",
       "      <td>1.899144</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Beautiful Mind, A (2001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175</td>\n",
       "      <td>15488</td>\n",
       "      <td>[2, 6]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.708734</td>\n",
       "      <td>1.899144</td>\n",
       "      <td>4.0</td>\n",
       "      <td>While You Were Sleeping (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2371</td>\n",
       "      <td>15488</td>\n",
       "      <td>[2, 8]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.213532</td>\n",
       "      <td>1.899144</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Police Academy 6: City Under Siege (1989)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1580</td>\n",
       "      <td>15488</td>\n",
       "      <td>[2]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.706598</td>\n",
       "      <td>1.899144</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Major League (1989)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>471</td>\n",
       "      <td>15488</td>\n",
       "      <td>[3, 11]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.710984</td>\n",
       "      <td>1.899144</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Thomas Crown Affair, The (1999)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId  userId   genres  rating_b  TE_movieId_rating  userId_count  \\\n",
       "0       79   15488   [1, 6]         1           0.845424      1.899144   \n",
       "1      175   15488   [2, 6]         1           0.708734      1.899144   \n",
       "2     2371   15488   [2, 8]         0           0.213532      1.899144   \n",
       "3     1580   15488      [2]         1           0.706598      1.899144   \n",
       "4      471   15488  [3, 11]         1           0.710984      1.899144   \n",
       "\n",
       "   rating                                      title  \n",
       "0     5.0                   Beautiful Mind, A (2001)  \n",
       "1     4.0             While You Were Sleeping (1995)  \n",
       "2     1.0  Police Academy 6: City Under Siege (1989)  \n",
       "3     5.0                        Major League (1989)  \n",
       "4     4.0            Thomas Crown Affair, The (1999)  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = valid[valid['userId']==15488].reset_index(drop=True)\n",
    "batch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e14970",
   "metadata": {},
   "source": [
    "Filter out the columns that are not used in the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e77192aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = batch[['movieId', 'userId', 'genres', 'TE_movieId_rating','userId_count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4de513",
   "metadata": {},
   "source": [
    "We first need to pad the genres column to be able to create a dictionary of tensors to serve as input to `model.predict()`. We could also use NVTabular `ListSlice` op for that but since we will only process couple of lines we can do that with a function defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7c5c0fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "def padding(s, seq_length=10):\n",
    "    padding_len = seq_length - len(s)\n",
    "    padded = np.pad(s, (0, padding_len), 'constant', constant_values=(0))\n",
    "    return padded\n",
    "\n",
    "padded_genres = batch_input['genres'].apply(padding)\n",
    "batch_input.loc[:,'genres'] = padded_genres.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9e81a",
   "metadata": {},
   "source": [
    "Convert our dataframe to a dictionary of tensors to feed to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b9750a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_tensor_dict(df):\n",
    "    data = df.to_dict(\"list\")\n",
    "    return {key: tf.convert_to_tensor(value) for key, value in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "acc03660",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tensor = tf_tensor_dict(batch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f9fd66dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform prediction for userId 15488\n",
    "predictions = model.predict(batch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fdc012",
   "metadata": {},
   "source": [
    "The predictions are probabilities that shows the likelihood of a user liking a movie or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3a9ffca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_b</th>\n",
       "      <th>predict_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15488</td>\n",
       "      <td>79</td>\n",
       "      <td>Beautiful Mind, A (2001)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15488</td>\n",
       "      <td>175</td>\n",
       "      <td>While You Were Sleeping (1995)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.987507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15488</td>\n",
       "      <td>2371</td>\n",
       "      <td>Police Academy 6: City Under Siege (1989)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.110548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15488</td>\n",
       "      <td>1580</td>\n",
       "      <td>Major League (1989)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.984806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15488</td>\n",
       "      <td>471</td>\n",
       "      <td>Thomas Crown Affair, The (1999)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.992783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>15488</td>\n",
       "      <td>70</td>\n",
       "      <td>Léon: The Professional (a.k.a. The Professiona...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.997819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>15488</td>\n",
       "      <td>183</td>\n",
       "      <td>Amadeus (1984)</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>15488</td>\n",
       "      <td>3297</td>\n",
       "      <td>Never Say Never Again (1983)</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.982946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>15488</td>\n",
       "      <td>1062</td>\n",
       "      <td>Transporter, The (2002)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.985573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>15488</td>\n",
       "      <td>110</td>\n",
       "      <td>L.A. Confidential (1997)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  movieId                                              title  \\\n",
       "0    15488       79                           Beautiful Mind, A (2001)   \n",
       "1    15488      175                     While You Were Sleeping (1995)   \n",
       "2    15488     2371          Police Academy 6: City Under Siege (1989)   \n",
       "3    15488     1580                                Major League (1989)   \n",
       "4    15488      471                    Thomas Crown Affair, The (1999)   \n",
       "..     ...      ...                                                ...   \n",
       "69   15488       70  Léon: The Professional (a.k.a. The Professiona...   \n",
       "70   15488      183                                     Amadeus (1984)   \n",
       "71   15488     3297                       Never Say Never Again (1983)   \n",
       "72   15488     1062                            Transporter, The (2002)   \n",
       "73   15488      110                           L.A. Confidential (1997)   \n",
       "\n",
       "    rating  rating_b  predict_proba  \n",
       "0      5.0         1       0.999110  \n",
       "1      4.0         1       0.987507  \n",
       "2      1.0         0       0.110548  \n",
       "3      5.0         1       0.984806  \n",
       "4      4.0         1       0.992783  \n",
       "..     ...       ...            ...  \n",
       "69     5.0         1       0.997819  \n",
       "70     4.5         1       0.998248  \n",
       "71     3.5         1       0.982946  \n",
       "72     4.0         1       0.985573  \n",
       "73     5.0         1       0.998266  \n",
       "\n",
       "[74 rows x 6 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['predict_proba'] = predictions\n",
    "batch[['userId', 'movieId', 'title','rating', 'rating_b', 'predict_proba']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899e92a",
   "metadata": {},
   "source": [
    "Let's find the top-5 movies that can be recommended to the user `15488` based on prediction probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f7aa6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked = np.argsort(predictions, axis=0)\n",
    "indices = ranked[::-1][:5].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8ad55be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>predict_proba</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>15488</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.999110</td>\n",
       "      <td>Beautiful Mind, A (2001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>213</td>\n",
       "      <td>15488</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.999062</td>\n",
       "      <td>Lock, Stock &amp; Two Smoking Barrels (1998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>15488</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.998943</td>\n",
       "      <td>Forrest Gump (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>73</td>\n",
       "      <td>15488</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.998461</td>\n",
       "      <td>Goodfellas (1990)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>26</td>\n",
       "      <td>15488</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.998405</td>\n",
       "      <td>Apollo 13 (1995)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    movieId  userId  rating  predict_proba  \\\n",
       "0        79   15488     5.0       0.999110   \n",
       "67      213   15488     5.0       0.999062   \n",
       "16        2   15488     5.0       0.998943   \n",
       "33       73   15488     4.5       0.998461   \n",
       "68       26   15488     5.0       0.998405   \n",
       "\n",
       "                                       title  \n",
       "0                   Beautiful Mind, A (2001)  \n",
       "67  Lock, Stock & Two Smoking Barrels (1998)  \n",
       "16                       Forrest Gump (1994)  \n",
       "33                         Goodfellas (1990)  \n",
       "68                          Apollo 13 (1995)  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.iloc[indices, :][['movieId', 'userId', 'rating', 'predict_proba', 'title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c6e349",
   "metadata": {},
   "source": [
    "We can see that the model predicted top-5 movies with high confidence and user's actual ratings for these movies correlates with the model prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
