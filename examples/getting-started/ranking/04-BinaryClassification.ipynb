{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157c84e7",
   "metadata": {},
   "source": [
    "## MERLIN MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb91495e",
   "metadata": {},
   "source": [
    "Recommender Systems (RecSys) attempt to accurately predict users' preferences by supporting users in the process of finding and selecting products (items) from a given catalog, thus enhance their engagement and overall satisfaction with online services. Several traditional (e.g. collaborative filtering) and Deep Learning-based RecSys models have been built and used in industry within the last decade. Recently, deep learning based recommender systems have become the norm for large scale industry applications. In contrast to the other domains, DL recommender models aren’t able to effectively leverage the parallel compute and high memory bandwidth that GPUs have to offer. To address the challenges in preprocessing, training and deploying recommender systems at scale, NVIDIA developed an open source framework, called [Merlin](https://github.com/NVIDIA-Merlin). One of the libraries of the Merlin framework is Merlin Models that is being developed to provide all the necessary tools to design an end-to-end RecSys pipeline, offering flexibility at each stage: multiple inputs processing/representation modules, different layers for designing the model’s architecture, different prediction heads, loss functions, different negative sampling techniques etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be7ff3",
   "metadata": {},
   "source": [
    "The goal of the `Merlin Models` library is make it easy for users in industry or academia to train and deploy recommender models with best practices baked into the library. This will let users in industry easily train standard models against their own dataset, getting high performance GPU accelerated models into production. This will also let researchers to build custom models by incorporating standard components of deep learning recommender models, and then benchmark their new models on example offline datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caa2e49",
   "metadata": {},
   "source": [
    "### Core features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc56f06",
   "metadata": {},
   "source": [
    "- Support for diverse input types (e.g. categorical and continuous), architectures (e.g. two-tower or sequential) or tasks (e.g. binary, multi-class classification, multi-task)\n",
    "- Flexible building blocks to define a broad range of models with various prediction tasks, loss functions and negative sampling techniques \n",
    "- Flexible APIs targeted to both production and research\n",
    "- Unified API enables users to create models in TensorFlow or PyTorch.\n",
    "- GPU-optimized data-loading, model training and serving\n",
    "- Deep integration with NVTabular for ETL and model serving\n",
    "- Multi-task learning is a first-class citizen\n",
    "- State-of-the-art negative sampling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3901b",
   "metadata": {},
   "source": [
    "### Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158113e5",
   "metadata": {},
   "source": [
    "In this introductory notebook we aim at\n",
    "\n",
    "- introducing the building blocks of Merlin Models library\n",
    "- showing how seamlessly integrate NVTabular and Merlin Models libraries\n",
    "- training DL-based recommender models with optimized Tensorflow data loaders with only a few lines of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983d921e",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9072154e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-06 15:08:35.352097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import nvtabular as nvt\n",
    "import numpy as np\n",
    "\n",
    "from nvtabular.loader.tensorflow import KerasSequenceLoader\n",
    "\n",
    "import merlin_models.tf as ml\n",
    "from merlin_standard_lib import Schema, Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a54549",
   "metadata": {},
   "source": [
    "## ETL with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5a0a26",
   "metadata": {},
   "source": [
    "We use the [MovieLens25M](https://grouplens.org/datasets/movielens/25m/) dataset which is a popular dataset for recommender systems and is used in academic publications. In order to download and convert the raw dataset, run `01-Download-Convert.ipynb` notebook first. Here, we will use parquet files processed using [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular) library. Run the `02-ETL-with-NVTabular.ipynb` notebook to generate the input features and processed parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecb8ce7",
   "metadata": {},
   "source": [
    "Avoid Numba low occupancy warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8021673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import config\n",
    "config.CUDA_LOW_OCCUPANCY_WARNINGS = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab3672",
   "metadata": {},
   "source": [
    "We define our base input directory, containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceaaea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get(\n",
    "    \"INPUT_DATA_DIR\", os.path.expanduser(\"/workspace/data/movielens/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb1925",
   "metadata": {},
   "source": [
    "### Schema Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29cd5a7",
   "metadata": {},
   "source": [
    "Merlin Models library relies on a schema object to automatically build all necessary layers to represent, normalize and aggregate input features. As you can see below, `schema.pbtxt` is a protobuf file that contains metadata including statistics about features such as cardinality, min and max values and also tags features based on their characteristics and dtypes (e.g., categorical, continuous, list, integer).\n",
    "\n",
    "We have already generated our `schema.pbtxt` file in the previous notebook using NVTabular. Now we read this schema file to create a schema object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f5c7d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"movieId\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"movieId\"\n",
      "    min: 0\n",
      "    max: 56690\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"item_id\"\n",
      "    tag: \"item\"\n",
      "    tag: \"categorical\"\n",
      "    extra_metadata {\n",
      "      type_url: \"type.googleapis.com/google.protobuf.Struct\"\n",
      "      value: \"\\n\\021\\n\\013num_buckets\\022\\002\\010\\000\\n\\033\\n\\016freq_threshold\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n\\025\\n\\010max_size\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n\\030\\n\\013start_index\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n2\\n\\010cat_path\\022&\\032$.//categories/unique.movieId.parquet\\nG\\n\\017embedding_sizes\\0224*2\\n\\030\\n\\013cardinality\\022\\t\\021\\000\\000\\000\\000@\\256\\353@\\n\\026\\n\\tdimension\\022\\t\\021\\000\\000\\000\\000\\000\\000\\200@\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"userId\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"userId\"\n",
      "    min: 0\n",
      "    max: 162542\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"user_id\"\n"
     ]
    }
   ],
   "source": [
    "from merlin_standard_lib import Schema\n",
    "SCHEMA_PATH = \"/workspace/data/movielens/train/schema.pbtxt\"\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)\n",
    "!head -30 $SCHEMA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd51d5",
   "metadata": {},
   "source": [
    "## Building an MLP with Merlin Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c5e9c9",
   "metadata": {},
   "source": [
    "Let's start with building an MLP model to train a binary classification task using Merlin Models library. As a starting point, we are goind to use only `['movieId', 'userId', 'genres']` input features. Note that our target column is `rating_b` which is created in the `02-ETL-with-NVTabular` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6047e",
   "metadata": {},
   "source": [
    "We can easily remove the features we do not want to use as input to the model with `remove_by_name` property of the schema object, and create a new schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30b66e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema.remove_by_name(['rating', 'title', 'TE_movieId_rating', 'userId_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2c42200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId', 'userId', 'genres']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef1f071",
   "metadata": {},
   "source": [
    "Below we build a TF MLP model using three main blocks:\n",
    "- InputBlock\n",
    "- MLPBlock\n",
    "- BinaryClassificationTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default emb_dim is 64\n",
    "model = ml.Model(\n",
    "    ml.InputBlock(schema), \n",
    "    ml.MLPBlock([64, 32]),\n",
    "    ml.BinaryClassificationTask(\"rating_b\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb64aa2",
   "metadata": {},
   "source": [
    "### Define Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594692a",
   "metadata": {},
   "source": [
    "We're ready to start training. We'll use the NVTabular `KerasSequenceLoader` for reading chunks of parquet files. `KerasSequenceLoader` manages shuffling by loading in chunks of data from different parts of the full dataset, concatenating them and then shuffling, then iterating through this super-chunk sequentially in batches. The number of \"parts\" of the dataset that get sample, or \"partitions\", is controlled by the `parts_per_chunk` kwarg, while the size of each one of these parts is controlled by the `buffer_size` kwarg, which refers to a fraction of available GPU memory (you can read more about it [here](https://nvidia-merlin.github.io/NVTabular/main/training/tensorflow.html) and [here](https://nvidia-merlin.github.io/NVTabular/main/api/tensorflow_dataloader.html?highlight=kerassequence#nvtabular.loader.tensorflow.KerasSequenceLoader)). Using more chunks leads to better randomness, especially at the epoch level where physically disparate samples can be brought into the same batch, but can impact throughput if you use too many. In any case, the speed of the parquet reader makes feasible buffer sizes much larger.\n",
    "\n",
    "Note that `genres` column is a multi-hot column and it is fed to dataloader as a sparse tensor and then it is converted to dense represantation. Based on our analysis, genres column has max 10 sequence of entries. So we will set the sequence length for the multi-hot columns as 10 in the `sparse_feature_max` dictionary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00ceebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and continuous columns\n",
    "x_cat_names, x_cont_names = ['userId', 'movieId', 'genres'], []\n",
    "\n",
    "# dictionary representing max sequence length for each column\n",
    "sparse_features_max = {'genres': 10}\n",
    "\n",
    "def get_dataloader(paths_or_dataset, batch_size=4096):\n",
    "    dataloader = KerasSequenceLoader(\n",
    "        paths_or_dataset,\n",
    "        batch_size=batch_size,\n",
    "        label_names=['rating_b'],\n",
    "        cat_names=x_cat_names,\n",
    "        cont_names=x_cont_names,\n",
    "        sparse_names=list(sparse_features_max.keys()),\n",
    "        sparse_max=sparse_features_max,\n",
    "        sparse_as_dense=True,\n",
    "    )\n",
    "    return dataloader.map(lambda X, y: (X, tf.reshape(y, (-1,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be6ee404",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"/workspace/data/movielens/\")\n",
    "train_paths = glob.glob(os.path.join(OUTPUT_DIR, \"train/*.parquet\"))\n",
    "eval_paths = glob.glob(os.path.join(OUTPUT_DIR, \"valid/*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2a1933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model.compile(optimizer=\"adam\", run_eagerly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*'*20)\n",
    "print(\"Launch training\")\n",
    "print('*'*20 + '\\n')\n",
    "train_loader = get_dataloader(train_paths) \n",
    "losses = model.fit(train_loader, epochs=1)\n",
    "model.reset_metrics()\n",
    "\n",
    "# Evaluate\n",
    "print('*'*20 + '\\n')\n",
    "print(\"Start evaluation\")\n",
    "eval_loader = get_dataloader(eval_paths) \n",
    "eval_metrics = model.evaluate(eval_loader, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572c371",
   "metadata": {},
   "source": [
    "Now we will build the same MLP model with `connect` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62f819bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ml.InputBlock(schema).connect(ml.MLPBlock([64, 32]), ml.BinaryClassificationTask(\"rating_b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7321b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", run_eagerly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef3817b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Launch training\n",
      "********************\n",
      "\n",
      "4883/4883 [==============================] - 171s 32ms/step - rating_b/binary_classification_task/precision: 0.7617 - rating_b/binary_classification_task/recall: 0.8614 - rating_b/binary_classification_task/binary_accuracy: 0.7449 - rating_b/binary_classification_task/auc: 0.8020 - loss: 0.5140 - regularization_loss: 0.0000e+00 - total_loss: 0.5140\n",
      "********************\n",
      "\n",
      "Start evaluation\n",
      "1221/1221 [==============================] - 45s 26ms/step - rating_b/binary_classification_task/precision: 0.7720 - rating_b/binary_classification_task/recall: 0.8688 - rating_b/binary_classification_task/binary_accuracy: 0.7574 - rating_b/binary_classification_task/auc: 0.8201 - loss: 0.4941 - regularization_loss: 0.0000e+00 - total_loss: 0.4941 21s - rating_b/binary_classification_task/precision: 0.7718 - rating_b/binary_classification_task/recall: 0.8\n"
     ]
    }
   ],
   "source": [
    "print('*'*20)\n",
    "print(\"Launch training\")\n",
    "print('*'*20 + '\\n')\n",
    "train_loader = get_dataloader(train_paths) \n",
    "losses = model.fit(train_loader, epochs=1)\n",
    "model.reset_metrics()\n",
    "\n",
    "# Evaluate\n",
    "print('*'*20 + '\\n')\n",
    "print(\"Start evaluation\")\n",
    "eval_loader = get_dataloader(eval_paths) \n",
    "eval_metrics = model.evaluate(eval_loader, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015b938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9204e88",
   "metadata": {},
   "source": [
    "## 3rd Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d49b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default emb_dim is 64\n",
    "\n",
    "block1 = ml.MLPBlock([256, 64])\n",
    "block2 = ml.MLPBlock([128, 64])\n",
    "\n",
    "body = ml.InputBlock(schema).connect_branch(block1, block2, aggregation='concat')\n",
    "\n",
    "model = body.connect(ml.BinaryClassificationTask(\"rating_b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4f94117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model.compile(optimizer=\"adam\", run_eagerly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7408b00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Launch training\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 16:37:06.049981: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2021-12-17 16:37:07.282600: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Block.parse of <class 'merlin_models.tf.core.Block'>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: annotated name 'output' can't be nonlocal (tmp4cg7dpwn.py, line 36)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Block.parse of <class 'merlin_models.tf.core.Block'>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: annotated name 'output' can't be nonlocal (tmp4cg7dpwn.py, line 36)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "4652/4883 [===========================>..] - ETA: 7s - rating_b/binary_classification_task/precision: 0.7616 - rating_b/binary_classification_task/recall: 0.8612 - rating_b/binary_classification_task/binary_accuracy: 0.7447 - rating_b/binary_classification_task/auc: 0.8018 - loss: 0.5142 - regularization_loss: 0.0000e+00 - total_loss: 0.5142  - ETA: 1:04 - rating_b/binar"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numba/cuda/compiler.py:865: NumbaPerformanceWarning: Grid size (9) < 2 * SM count (160) will likely result in GPU under utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4883/4883 [==============================] - 174s 32ms/step - rating_b/binary_classification_task/precision: 0.7623 - rating_b/binary_classification_task/recall: 0.8613 - rating_b/binary_classification_task/binary_accuracy: 0.7453 - rating_b/binary_classification_task/auc: 0.8027 - loss: 0.5132 - regularization_loss: 0.0000e+00 - total_loss: 0.5132\n",
      "********************\n",
      "\n",
      "Start evaluation\n",
      "1004/1221 [=======================>......] - ETA: 6s - rating_b/binary_classification_task/precision: 0.7726 - rating_b/binary_classification_task/recall: 0.8685 - rating_b/binary_classification_task/binary_accuracy: 0.7579 - rating_b/binary_classification_task/auc: 0.8208 - loss: 0.4933 - regularization_loss: 0.0000e+00 - total_loss: 0.4933  ETA: 21s - rating_b/binary_classification_task/precision: 0.7724 - rating_b/binary_classification_task/recall: 0.8686 - rating_b/binar"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numba/cuda/compiler.py:865: NumbaPerformanceWarning: Grid size (8) < 2 * SM count (160) will likely result in GPU under utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1221/1221 [==============================] - 45s 26ms/step - rating_b/binary_classification_task/precision: 0.7726 - rating_b/binary_classification_task/recall: 0.8685 - rating_b/binary_classification_task/binary_accuracy: 0.7579 - rating_b/binary_classification_task/auc: 0.8207 - loss: 0.4933 - regularization_loss: 0.0000e+00 - total_loss: 0.4933\n"
     ]
    }
   ],
   "source": [
    "print('*'*20)\n",
    "print(\"Launch training\")\n",
    "print('*'*20 + '\\n')\n",
    "train_loader = get_dataloader(train_paths) \n",
    "losses = model.fit(train_loader, epochs=1)\n",
    "model.reset_metrics()\n",
    "\n",
    "# Evaluate\n",
    "print('*'*20 + '\\n')\n",
    "print(\"Start evaluation\")\n",
    "eval_loader = get_dataloader(eval_paths) \n",
    "eval_metrics = model.evaluate(eval_loader, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e35707",
   "metadata": {},
   "source": [
    "Recommender systems need to be highly scalable while matching millions of items with billions of users with ~100 milliseconds latency. The scalability requirement has led two-stage recommender systems to be widely used by industry. A two-stage recommender system consists of efficient candidate generation model(s) in the first stage and a more powerful ranking model in the second stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d01417",
   "metadata": {},
   "source": [
    "### DLRM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "972a97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema.remove_by_name(['rating', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a22e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId', 'userId', 'genres', 'rating_b']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ab712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and continuous columns\n",
    "x_cat_names, x_cont_names = ['userId', 'movieId', 'genres'], []\n",
    "\n",
    "# dictionary representing max sequence length for each column\n",
    "sparse_features_max = {'genres': 10}\n",
    "\n",
    "def get_dataloader(paths_or_dataset, batch_size=20):\n",
    "    dataloader = KerasSequenceLoader(\n",
    "        paths_or_dataset,\n",
    "        batch_size=batch_size,\n",
    "        label_names=['rating_b'],\n",
    "        cat_names=x_cat_names,\n",
    "        cont_names=x_cont_names,\n",
    "        sparse_names=list(sparse_features_max.keys()),\n",
    "        sparse_max=sparse_features_max,\n",
    "        sparse_as_dense=True,\n",
    "    )\n",
    "    return dataloader.map(lambda X, y: (X, tf.reshape(y, (-1,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc618701",
   "metadata": {},
   "source": [
    "in batch negative sampling it will create the number of negatives wrt batch_size in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac03848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"/workspace/data/movielens/\")\n",
    "train_paths = glob.glob(os.path.join(OUTPUT_DIR, \"train/*.parquet\"))\n",
    "eval_paths = glob.glob(os.path.join(OUTPUT_DIR, \"valid/*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c7e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_dataloader(train_paths) \n",
    "# losses = model.fit(train_loader, epochs=1)\n",
    "# model.reset_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d70b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# music_streaming_data._schema = music_streaming_data.schema.remove_by_tag(Tag.TARGETS)\n",
    "import tensorflow as tf\n",
    "\n",
    "two_tower = ml.TwoTowerBlock(schema, query_tower=ml.MLPBlock([512, 256]))\n",
    "model = two_tower.connect(ml.ItemRetrievalTask(softmax_temperature=2))\n",
    "\n",
    "# output = model(music_streaming_data.tf_tensor_dict)\n",
    "# assert output is not None\n",
    "\n",
    "model.compile(optimizer=\"adam\", run_eagerly=False)\n",
    "losses = model.fit(train_loader, epochs=1)\n",
    "# assert len(losses.epoch) == num_epochs\n",
    "# assert all(measure >= 0 for metric in losses.history for measure in losses.history[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab647b",
   "metadata": {},
   "source": [
    "it is multiclass because it is one positive and multi-negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd81644",
   "metadata": {},
   "source": [
    "### NCF example with Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0] --> TwoTower block\n",
    "\n",
    "# This is for the inference\n",
    "user_model = model[0]['query'] \n",
    "\n",
    "user_model(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f382dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
