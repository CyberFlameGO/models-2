{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94662dcc",
   "metadata": {},
   "source": [
    "## MERLIN MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35143f",
   "metadata": {},
   "source": [
    "Recommender Systems (RecSys) attempt to accurately predict users' preferences by supporting users in the process of finding and selecting products (items) from a given catalog, thus enhance their engagement and overall satisfaction with online services. Several traditional (e.g. collaborative filtering) and Deep Learning-based RecSys models have been built and used in industry within the last decade. Recently, deep learning based recommender systems have become the norm for large scale industry applications. In contrast to the other domains, DL recommender models aren’t able to effectively leverage the parallel compute and high memory bandwidth that GPUs have to offer. To address the challenges in preprocessing, training and deploying recommender systems at scale, NVIDIA developed an open source framework, called [Merlin](https://github.com/NVIDIA-Merlin). One of the libraries of the Merlin framework is Merlin Models that is being developed to provide all the necessary tools to design an end-to-end RecSys pipeline, offering flexibility at each stage: multiple inputs processing/representation modules, different layers for designing the model’s architecture, different prediction heads, loss functions, different negative sampling techniques etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f7471",
   "metadata": {},
   "source": [
    "The goal of the `Merlin Models` library is make it easy for users in industry or academia to train and deploy recommender models with best practices baked into the library. This will let users in industry easily train standard models against their own dataset, getting high performance GPU accelerated models into production. This will also let researchers to build custom models by incorporating standard components of deep learning recommender models, and then benchmark their new models on example offline datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cdd9a2",
   "metadata": {},
   "source": [
    "### Core features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a776e5a0",
   "metadata": {},
   "source": [
    "- Support for diverse input types (e.g. categorical and continuous), architectures (e.g. two-tower or sequential) or tasks (e.g. binary, multi-class classification, multi-task)\n",
    "- Flexible building blocks to define a broad range of models with various prediction tasks, loss functions and negative sampling techniques \n",
    "- Flexible APIs targeted to both production and research\n",
    "- Unified API enables users to create models in TensorFlow or PyTorch.\n",
    "- GPU-optimized data-loading, model training and serving\n",
    "- Deep integration with NVTabular for ETL and model serving\n",
    "- Multi-task learning is a first-class citizen\n",
    "- State-of-the-art negative sampling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a3dc3c",
   "metadata": {},
   "source": [
    "### Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758ab13",
   "metadata": {},
   "source": [
    "In this introductory notebook we aim at\n",
    "\n",
    "- introducing the building blocks of Merlin Models library\n",
    "- showing how seamlessly integrate NVTabular and Merlin Models libraries\n",
    "- training DL-based recommender models with optimized Tensorflow data loaders with only a few lines of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee67016",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ca29a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 13:28:19.053220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import nvtabular as nvt\n",
    "import numpy as np\n",
    "\n",
    "from nvtabular.loader.tensorflow import KerasSequenceLoader\n",
    "\n",
    "import merlin_models.tf as ml\n",
    "from merlin_standard_lib import Schema, Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a217e2",
   "metadata": {},
   "source": [
    "## ETL with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d9719",
   "metadata": {},
   "source": [
    "We use the [MovieLens25M](https://grouplens.org/datasets/movielens/25m/) dataset which is a popular dataset for recommender systems and is used in academic publications. In order to download and convert the raw dataset, run `01-Download-Convert.ipynb` notebook first. Here, we will use parquet files processed using [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular) library. Run the `02-ETL-with-NVTabular.ipynb` notebook to generate the input features and processed parquet files. The ETL notebook explains all the preprocessing and feature engineering steps in details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5e3c4",
   "metadata": {},
   "source": [
    "Avoid Numba low occupancy warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37016251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import config\n",
    "config.CUDA_LOW_OCCUPANCY_WARNINGS = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e1017a",
   "metadata": {},
   "source": [
    "We define our base input directory, containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd9530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get(\n",
    "    \"INPUT_DATA_DIR\", os.path.expanduser(\"/workspace/data/movielens/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b78a4",
   "metadata": {},
   "source": [
    "### Schema Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8f8ff",
   "metadata": {},
   "source": [
    "Merlin Models library relies on a schema object to automatically build all necessary layers to represent, normalize and aggregate input features. As you can see below, `schema.pbtxt` is a protobuf file that contains metadata including statistics about features such as cardinality, min and max values and also tags features based on their characteristics and dtypes (e.g., categorical, continuous, list, integer).\n",
    "\n",
    "We have already generated our `schema.pbtxt` file in the previous notebook using `NVTabular`. Now we read this schema file to create a schema object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045620f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"movieId\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"movieId\"\n",
      "    min: 0\n",
      "    max: 56690\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"item_id\"\n",
      "    tag: \"item\"\n",
      "    tag: \"categorical\"\n",
      "    extra_metadata {\n",
      "      type_url: \"type.googleapis.com/google.protobuf.Struct\"\n",
      "      value: \"\\n\\021\\n\\013num_buckets\\022\\002\\010\\000\\n\\033\\n\\016freq_threshold\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n\\025\\n\\010max_size\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n\\030\\n\\013start_index\\022\\t\\021\\000\\000\\000\\000\\000\\000\\000\\000\\n2\\n\\010cat_path\\022&\\032$.//categories/unique.movieId.parquet\\nG\\n\\017embedding_sizes\\0224*2\\n\\030\\n\\013cardinality\\022\\t\\021\\000\\000\\000\\000@\\256\\353@\\n\\026\\n\\tdimension\\022\\t\\021\\000\\000\\000\\000\\000\\000\\200@\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"userId\"\n",
      "  type: INT\n",
      "  int_domain {\n",
      "    name: \"userId\"\n",
      "    min: 0\n",
      "    max: 162542\n",
      "    is_categorical: true\n",
      "  }\n",
      "  annotation {\n",
      "    tag: \"user_id\"\n"
     ]
    }
   ],
   "source": [
    "from merlin_standard_lib import Schema\n",
    "SCHEMA_PATH = \"/workspace/data/movielens/train/schema.pbtxt\"\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)\n",
    "!head -30 $SCHEMA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83df4201",
   "metadata": {},
   "source": [
    "## Building an MLP with Merlin Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd2aeb",
   "metadata": {},
   "source": [
    "Let's start with building an MLP model to train a binary classification task using Merlin Models library. As a starting point, we are goind to use only `['movieId', 'userId', 'genres']` input features. Note that our target column is `rating_b` which is created in the `02-ETL-with-NVTabular` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f203f9",
   "metadata": {},
   "source": [
    "We can easily remove the features we do not want to use as input to the model with `remove_by_name` property of the schema object, and create a new schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d1eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema.remove_by_name(['rating', 'title', 'TE_movieId_rating', 'userId_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64d03ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId', 'userId', 'genres', 'rating_b']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4372237",
   "metadata": {},
   "source": [
    "Below we build a TF MLP model using three main blocks:\n",
    "- [InputBlock](https://github.com/NVIDIA-Merlin/models/blob/main/merlin_models/tf/block/inputs.py) is the entry block of the model that accepts `schema` object and other arguments like `aggregation`, `continuous projection`, `embedding options`. This function creates continuous and embedding layers, and returns a single output of `TabularData` which is a dictionary of tensors via [ParallelBlock](https://github.com/NVIDIA-Merlin/models/blob/main/merlin_models/tf/core.py#L1184) class by merging multiple layers or TabularModules. Note that in this example we do not have numerical features, we only have categorical features.\n",
    "- [MLPBlock](https://github.com/NVIDIA-Merlin/models/blob/main/merlin_models/tf/block/mlp.py) is to define a Multi-layer perceptron block, where each dimension is used to create a Dense layer. In addition to `dimension` argument we can also feed `activation`, `use_bais`, `dropout`, etc. arguments to the `MLPBlock` function.\n",
    "- [BinaryClassificationTask](https://github.com/NVIDIA-Merlin/models/blob/main/merlin_models/tf/prediction/classification.py#L30) supports the binary prediction task. Merlin Models library also support other predictions tasks, like next-item prediction and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75001a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelBlock(\n",
       "  (parallel_layers): Dict(\n",
       "    (categorical): EmbeddingFeatures(\n",
       "      (feature_config): Dict(\n",
       "        (movieId): TableConfig(vocabulary_size=56691, dim=64, initializer=None, optimizer=None, combiner='mean', name='movieId')\n",
       "        (userId): TableConfig(vocabulary_size=162543, dim=64, initializer=None, optimizer=None, combiner='mean', name='userId')\n",
       "        (genres): TableConfig(vocabulary_size=22, dim=64, initializer=None, optimizer=None, combiner='mean', name='genres')\n",
       "      )\n",
       "      (_pre): SequentialBlock(\n",
       "        (layers): List(\n",
       "          (0): Filter(\n",
       "            (feature_names): List(\n",
       "              (0): 'movieId'\n",
       "              (1): 'userId'\n",
       "              (2): 'genres'\n",
       "            )\n",
       "          )\n",
       "          (1): AsSparseFeatures()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.InputBlock(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2f5f5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default emb_dim is 64\n",
    "model = ml.Model(\n",
    "    ml.InputBlock(schema), \n",
    "    ml.MLPBlock([64, 32]),\n",
    "    ml.BinaryClassificationTask(\"rating_b\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca42d3",
   "metadata": {},
   "source": [
    "As you can notice we start with InputBlock, then create an MLP block, and then finally, we connect our MLP block to the BinaryClassificationTask head to be able to do binary classification, and create our model class. Basically, this is similar to a linear stacking of layers into a tf.keras.Model as done with tf.keras.Sequential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b420d2",
   "metadata": {},
   "source": [
    "### Define Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045bae41",
   "metadata": {},
   "source": [
    "We're ready to start training. We'll use the NVTabular `KerasSequenceLoader` for reading chunks of parquet files. `KerasSequenceLoader` manages shuffling by loading in chunks of data from different parts of the full dataset, concatenating them and then shuffling, then iterating through this super-chunk sequentially in batches. The number of \"parts\" of the dataset that get sample, or \"partitions\", is controlled by the `parts_per_chunk` kwarg, while the size of each one of these parts is controlled by the `buffer_size` kwarg, which refers to a fraction of available GPU memory (you can read more about it [here](https://nvidia-merlin.github.io/NVTabular/main/training/tensorflow.html) and [here](https://nvidia-merlin.github.io/NVTabular/main/api/tensorflow_dataloader.html?highlight=kerassequence#nvtabular.loader.tensorflow.KerasSequenceLoader)). Using more chunks leads to better randomness, especially at the epoch level where physically disparate samples can be brought into the same batch, but can impact throughput if you use too many. In any case, the speed of the parquet reader makes feasible buffer sizes much larger.\n",
    "\n",
    "Note that `genres` column is a multi-hot column and it is fed to dataloader as a sparse tensor and then it is converted to dense represantation. Based on our analysis, genres column has max 10 sequence of entries. So we will set the sequence length for the multi-hot columns as 10 in the `sparse_feature_max` dictionary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac19296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and continuous columns\n",
    "x_cat_names, x_cont_names = ['userId', 'movieId', 'genres'], []\n",
    "\n",
    "# dictionary representing max sequence length for each column\n",
    "sparse_features_max = {'genres': 10}\n",
    "\n",
    "def get_dataloader(paths_or_dataset, batch_size=4096):\n",
    "    dataloader = KerasSequenceLoader(\n",
    "        paths_or_dataset,\n",
    "        batch_size=batch_size,\n",
    "        label_names=['rating_b'],\n",
    "        cat_names=x_cat_names,\n",
    "        cont_names=x_cont_names,\n",
    "        sparse_names=list(sparse_features_max.keys()),\n",
    "        sparse_max=sparse_features_max,\n",
    "        sparse_as_dense=True,\n",
    "    )\n",
    "    return dataloader.map(lambda X, y: (X, tf.reshape(y, (-1,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a5bce",
   "metadata": {},
   "source": [
    "Define the parquet file paths for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8b3a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"/workspace/data/movielens/\")\n",
    "train_paths = glob.glob(os.path.join(OUTPUT_DIR, \"train/*.parquet\"))\n",
    "eval_paths = glob.glob(os.path.join(OUTPUT_DIR, \"valid/*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65ec032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model.compile(optimizer=\"adam\", run_eagerly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae15e6",
   "metadata": {},
   "source": [
    "In order to train and evaluate our model we are using `.fit()` and `.evaluate()` methods as done in `tf.keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd487538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Launch training\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 13:28:27.415716: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-01-07 13:28:28.510793: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Block.parse of <class 'merlin_models.tf.core.Block'>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: annotated name 'output' can't be nonlocal (tmpv584qd_r.py, line 36)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Block.parse of <class 'merlin_models.tf.core.Block'>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: annotated name 'output' can't be nonlocal (tmpv584qd_r.py, line 36)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "4883/4883 [==============================] - 173s 32ms/step - rating_b/binary_classification_task/precision: 0.7616 - rating_b/binary_classification_task/recall: 0.8612 - rating_b/binary_classification_task/binary_accuracy: 0.7447 - rating_b/binary_classification_task/auc: 0.8021 - loss: 0.5139 - regularization_loss: 0.0000e+00 - total_loss: 0.5139\n",
      "********************\n",
      "\n",
      "Start evaluation\n",
      "1221/1221 [==============================] - 45s 26ms/step - rating_b/binary_classification_task/precision: 0.7738 - rating_b/binary_classification_task/recall: 0.8649 - rating_b/binary_classification_task/binary_accuracy: 0.7574 - rating_b/binary_classification_task/auc: 0.8202 - loss: 0.4940 - regularization_loss: 0.0000e+00 - total_loss: 0.4940 20s - rating_b/binary_classification_task/precision: 0.7736 - rating_b/binary_classification_task/recal\n"
     ]
    }
   ],
   "source": [
    "print('*'*20)\n",
    "print(\"Launch training\")\n",
    "print('*'*20 + '\\n')\n",
    "train_loader = get_dataloader(train_paths) \n",
    "losses = model.fit(train_loader, epochs=1)\n",
    "model.reset_metrics()\n",
    "\n",
    "# Evaluate\n",
    "print('*'*20 + '\\n')\n",
    "print(\"Start evaluation\")\n",
    "eval_loader = get_dataloader(eval_paths) \n",
    "eval_metrics = model.evaluate(eval_loader, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab31995",
   "metadata": {},
   "source": [
    "Now we will build the same MLP model with `connect` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c023b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ml.InputBlock(schema).connect(ml.MLPBlock([64, 32]), ml.BinaryClassificationTask(\"rating_b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc7380a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", run_eagerly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37fa5e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Launch training\n",
      "********************\n",
      "\n",
      "4883/4883 [==============================] - 171s 32ms/step - rating_b/binary_classification_task/precision: 0.7616 - rating_b/binary_classification_task/recall: 0.8613 - rating_b/binary_classification_task/binary_accuracy: 0.7448 - rating_b/binary_classification_task/auc: 0.8021 - loss: 0.5139 - regularization_loss: 0.0000e+00 - total_loss: 0.5139\n",
      "********************\n",
      "\n",
      "Start evaluation\n",
      "1221/1221 [==============================] - 44s 26ms/step - rating_b/binary_classification_task/precision: 0.7721 - rating_b/binary_classification_task/recall: 0.8677 - rating_b/binary_classification_task/binary_accuracy: 0.7571 - rating_b/binary_classification_task/auc: 0.8198 - loss: 0.4945 - regularization_loss: 0.0000e+00 - total_loss: 0.4945\n"
     ]
    }
   ],
   "source": [
    "print('*'*20)\n",
    "print(\"Launch training\")\n",
    "print('*'*20 + '\\n')\n",
    "train_loader = get_dataloader(train_paths) \n",
    "losses = model.fit(train_loader, epochs=1)\n",
    "model.reset_metrics()\n",
    "\n",
    "# Evaluate\n",
    "print('*'*20 + '\\n')\n",
    "print(\"Start evaluation\")\n",
    "eval_loader = get_dataloader(eval_paths) \n",
    "eval_metrics = model.evaluate(eval_loader, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49415070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
