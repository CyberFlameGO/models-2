{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e85f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545dba4b",
   "metadata": {},
   "source": [
    "## Training a DLRM model with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3bbbec",
   "metadata": {},
   "source": [
    "In the previous notebooks, we have downloaded the movielens data, converted it to parquet files and then used NVTabular library to process the data, join data frames, and create input features. In this notebook we will use NVIDIA Merlin Models library to build and train a Deep Learning Recommendation Model [(DLRM)](https://arxiv.org/abs/1906.00091) architecture originally proposed by Facebook in 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a169a4",
   "metadata": {},
   "source": [
    "Figure 1 illustrates DLRM architecture. The model was introduced as a personalization deep learning model that uses embeddings to process sparse features that represent categorical data and a multilayer perceptron (MLP) to process dense features, then interacts these features explicitly using the statistical techniques proposed in [here](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5694074)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ae5d2",
   "metadata": {},
   "source": [
    "![DLRM](images/DLRM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c58a9f",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c08d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import nvtabular\n",
    "import numpy as np\n",
    "\n",
    "import merlin_models.tf as ml\n",
    "from merlin_standard_lib import Schema, Tag\n",
    "\n",
    "from nvtabular.loader.tensorflow import KerasSequenceLoader\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "712c8a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "logging.disable(logging.WARNING) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823fa168",
   "metadata": {},
   "source": [
    "Merlin Models library relies on a `schema` object to automatically build all necessary layers to represent, normalize and aggregate input features. As you can see below, schema.pb is a protobuf file that contains metadata including statistics about features such as cardinality, min and max values and also tags features based on their characteristics and dtypes (e.g., categorical, continuous, list, integer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f95e1ce",
   "metadata": {},
   "source": [
    "We have already generated our `schema.pbtxt` file in the previous notebook using NVTabular. Not we read this schema file to create a `schema` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d98d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin_standard_lib import Schema\n",
    "SCHEMA_PATH = \"/workspace/data/movielens/train/schema.pbtxt\"\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)\n",
    "#!cat $SCHEMA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f0af65",
   "metadata": {},
   "source": [
    "## Define the Input module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19512b2d",
   "metadata": {},
   "source": [
    "Below we define our input block using the `ml.ContinuousEmbedding` function. The from_schema() method processes the schema and creates the necessary layers to represent features and aggregate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b68d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 00:33:48.339892: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-08 00:33:49.434830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "con_schema = schema.select_by_tag(Tag.CONTINUOUS)\n",
    "cat_schema = schema.select_by_tag(Tag.CATEGORICAL)\n",
    "\n",
    "top_block_inputs = {}\n",
    "\n",
    "top_block_inputs[\"continuous\"] = ml.ContinuousFeatures.from_schema(con_schema).connect(ml.MLPBlock([128, 64]))\n",
    "\n",
    "\n",
    "embedding_dim = 64\n",
    "top_block_inputs[\"categorical\"] = ml.EmbeddingFeatures.from_schema(\n",
    "    cat_schema, embedding_dim_default=embedding_dim\n",
    ")\n",
    "\n",
    "\n",
    "dot_product = ml.TabularBlock(aggregation=\"stack\").connect(ml.DotProductInteraction())\n",
    "top_block_outputs = (ml.merge(top_block_inputs).connect_with_shortcut\n",
    "                     (\n",
    "                         dot_product, shortcut_filter=ml.Filter(\"continuous\"), aggregation=\"concat\"\n",
    "                     ).connect(ml.MLPBlock([128, 64])\n",
    "                              )\n",
    "                    )\n",
    "model = top_block_outputs.connect(ml.BinaryClassificationTask(\"rating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8da2b737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId', 'userId', 'genres']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_schema.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f673e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c68e189",
   "metadata": {},
   "source": [
    "merge is going to return a dictionary of blocks for categs and continous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6822ed9f",
   "metadata": {},
   "source": [
    "if we remove the `cosine` aggregation, this will create a dictionary.. we can use ItemRetrivealTask and it will take care of the aggregation and negative sampling. current default is `in-batch-negative sampling\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f726cd",
   "metadata": {},
   "source": [
    "### Define Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a353540",
   "metadata": {},
   "source": [
    "We're ready to get trainin.. We'll use the NVTabular `KerasSequenceLoader` for reading chunks of parquet files. `KerasSequenceLoader` manages shuffling by loading in chunks of data from different parts of the full dataset, concatenating them and then shuffling, then iterating through this super-chunk sequentially in batches. The number of \"parts\" of the dataset that get sample, or \"partitions\", is controlled by the `parts_per_chunk` kwarg, while the size of each one of these parts is controlled by the `buffer_size` kwarg, which refers to a fraction of available GPU memory (you can read more about it [here](https://nvidia-merlin.github.io/NVTabular/main/training/tensorflow.html) and [here](https://nvidia-merlin.github.io/NVTabular/main/api/tensorflow_dataloader.html?highlight=kerassequence#nvtabular.loader.tensorflow.KerasSequenceLoader)). Using more chunks leads to better randomness, especially at the epoch level where physically disparate samples can be brought into the same batch, but can impact throughput if you use too many. In any case, the speed of the parquet reader makes feasible buffer sizes much larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02ae728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and continuous columns\n",
    "x_cat_names, x_cont_names = ['userId', 'movieId', 'genres'], ['TE_movieId_rating','userId_count']\n",
    "\n",
    "# dictionary representing max sequence length for each column\n",
    "sparse_features_max = {'genres': 20}\n",
    "\n",
    "def get_dataloader(paths_or_dataset, batch_size=4096):\n",
    "    dataloader = KerasSequenceLoader(\n",
    "        paths_or_dataset,\n",
    "        batch_size=batch_size,\n",
    "        label_names=['rating'],\n",
    "        cat_names=x_cat_names,\n",
    "        cont_names=x_cont_names,\n",
    "        sparse_names=list(sparse_features_max.keys()),\n",
    "        sparse_max=sparse_features_max,\n",
    "        sparse_as_dense=True,\n",
    "    )\n",
    "    return dataloader.map(lambda X, y: (X, tf.reshape(y, (-1,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27cd8767",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"/workspace/data/movielens/\")\n",
    "train_paths = glob.glob(os.path.join(OUTPUT_DIR, \"train/*.parquet\"))\n",
    "eval_paths = glob.glob(os.path.join(OUTPUT_DIR, \"valid/*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dffe74b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", run_eagerly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45682400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Launch training\n",
      "********************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-08 00:34:04.102660: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4650/4883 [===========================>..] - ETA: 8s - rating/binary_classification_task/precision: 0.7615 - rating/binary_classification_task/recall: 0.8609 - rating/binary_classification_task/binary_accuracy: 0.7445 - rating/binary_classification_task/auc: 0.8002 - loss: 0.5162 - regularization_loss: 0.0000e+00 - total_loss: 0.5162"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numba/cuda/compiler.py:865: NumbaPerformanceWarning: Grid size (10) < 2 * SM count (160) will likely result in GPU under utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4883/4883 [==============================] - 180s 35ms/step - rating/binary_classification_task/precision: 0.7620 - rating/binary_classification_task/recall: 0.8610 - rating/binary_classification_task/binary_accuracy: 0.7449 - rating/binary_classification_task/auc: 0.8010 - loss: 0.5153 - regularization_loss: 0.0000e+00 - total_loss: 0.5153\n",
      "Epoch 2/3\n",
      "4883/4883 [==============================] - 175s 35ms/step - rating/binary_classification_task/precision: 0.7794 - rating/binary_classification_task/recall: 0.8628 - rating/binary_classification_task/binary_accuracy: 0.7615 - rating/binary_classification_task/auc: 0.8261 - loss: 0.4869 - regularization_loss: 0.0000e+00 - total_loss: 0.4869\n",
      "Epoch 3/3\n",
      "4883/4883 [==============================] - 175s 35ms/step - rating/binary_classification_task/precision: 0.7895 - rating/binary_classification_task/recall: 0.8652 - rating/binary_classification_task/binary_accuracy: 0.7715 - rating/binary_classification_task/auc: 0.8395 - loss: 0.4704 - regularization_loss: 0.0000e+00 - total_loss: 0.4704\n",
      " 994/1221 [=======================>......] - ETA: 7s - rating/binary_classification_task/precision: 0.7811 - rating/binary_classification_task/recall: 0.8636 - rating/binary_classification_task/binary_accuracy: 0.7634 - rating/binary_classification_task/auc: 0.8266 - loss: 0.4886 - regularization_loss: 0.0000e+00 - total_loss: 0.4886"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numba/cuda/compiler.py:865: NumbaPerformanceWarning: Grid size (8) < 2 * SM count (160) will likely result in GPU under utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1221/1221 [==============================] - 46s 27ms/step - rating/binary_classification_task/precision: 0.7810 - rating/binary_classification_task/recall: 0.8635 - rating/binary_classification_task/binary_accuracy: 0.7633 - rating/binary_classification_task/auc: 0.8267 - loss: 0.4886 - regularization_loss: 0.0000e+00 - total_loss: 0.4886\n",
      "********************\n",
      "Eval results\n",
      "\n",
      "********************\n",
      "\n",
      " loss = 0.47167977690696716\n",
      " rating/binary_classification_task/auc = 0.8266725540161133\n",
      " rating/binary_classification_task/binary_accuracy = 0.7632789611816406\n",
      " rating/binary_classification_task/precision = 0.7810056805610657\n",
      " rating/binary_classification_task/recall = 0.8634868264198303\n",
      " regularization_loss = 0\n",
      " total_loss = 0.47167977690696716\n"
     ]
    }
   ],
   "source": [
    "print('*'*20)\n",
    "print(\"Launch training\")\n",
    "print('*'*20 + '\\n')\n",
    "train_loader = get_dataloader(train_paths) \n",
    "losses = model.fit(train_loader, epochs=3)\n",
    "model.reset_metrics()\n",
    "# Evaluate\n",
    "eval_loader = get_dataloader(eval_paths) \n",
    "eval_metrics = model.evaluate(eval_loader, return_dict=True)\n",
    "print('*'*20)\n",
    "print(\"Eval results\")\n",
    "print('\\n' + '*'*20 + '\\n')\n",
    "for key in sorted(eval_metrics.keys()):\n",
    "    print(\" %s = %s\" % (key, str(eval_metrics[key]))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
