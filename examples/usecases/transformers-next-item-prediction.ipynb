{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions anda\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80126e",
   "metadata": {},
   "source": [
    "Run the below cell once in a new TF 22.09 container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# cd /models && git pull && pip install .\n",
    "# cd /nvtabular && git pull && pip install .\n",
    "# cd /core && git pull && pip install .\n",
    "# cd /systems && git pull && pip install .\n",
    "\n",
    "# pip install tensorflow\n",
    "# pip install transformers==4.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1edd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I do not need this on my machine, but can be helpful if you encounter issues\n",
    "\n",
    "# import os\n",
    "# os.environ[\"FORCE_TF_AVAILABLE\"]=\"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d1452",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_models_entertainment-with-pretrained-embeddings/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Transformer-based architecture for next-item prediction task\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this use case we will train a Transformer-based architecture for next-item prediction task.\n",
    "\n",
    "We will use the [booking.com dataset](https://github.com/bookingcom/ml-dataset-mdt) to train a session-based model. The dataset contains 1,166,835 of anonymized hotel reservations in the train set and 378,667 in the test set. Each reservation is a part of a customer's trip (identified by `utrip_id`) which includes consecutive reservations.\n",
    "\n",
    "We will reshape the data to organize it into 'sessions'. Each session will be a full customer itinerary in chronological order. The goal will be to predict the city_id of the final reservation of each trip.\n",
    "\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "- Training a Transformer-based architecture for next-item prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cccd005",
   "metadata": {},
   "source": [
    "## Downloading and preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b619b",
   "metadata": {},
   "source": [
    "You can download the full dataset from GitHub [here](https://github.com/bookingcom/ml-dataset-mdt). Please place it alognside this notebook (or alternatively, change the `DATAPATH` to point to where it is located)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9dccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.core.dispatch import get_lib\n",
    "import numpy as np\n",
    "\n",
    "DATAPATH = 'ml-dataset-mdt'\n",
    "\n",
    "itineraries = get_lib().read_csv(f'{DATAPATH}/train_set.csv', parse_dates=['checkin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b7224",
   "metadata": {},
   "source": [
    "Each reservation has a unique `utrip_id`. During each trip a customer vists several destinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "itineraries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554eb309",
   "metadata": {},
   "source": [
    "We will limit the sequence length to between 2 and 10 trips. That will capture upwards of 95% datapoints!\n",
    "\n",
    "We don't want to train on trips that are shorter than two hops -- our model would not be able to learn much from such sequences. Additionally, such short sequences are uncharacteristically short for this dataset.\n",
    "\n",
    "Besides, training on unusually long or short sequences, that are far outside of the most common sequence length, might not be the best use of our compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TRIP_LENGTH = 10\n",
    "MIN_TRIP_LENGTH = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a0702c",
   "metadata": {},
   "source": [
    "Let us now split the data into a train and validation set based on trip ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "utrip_ids = itineraries.utrip_id.unique().sample(frac=1)\n",
    "len(utrip_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_utrip_ids = utrip_ids[:160_000]\n",
    "validation_set_utrip_ids = utrip_ids[160_000:]\n",
    "\n",
    "train_set = itineraries[itineraries.utrip_id.isin(train_set_utrip_ids)]\n",
    "validation_set = itineraries[itineraries.utrip_id.isin(validation_set_utrip_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cc3992",
   "metadata": {},
   "source": [
    "We can now begin with data preprocessing.\n",
    "\n",
    "We will combine trips into \"sessions\", discard trips that are either too short or too long and calculate total trip length in stops.\n",
    "\n",
    "We will use nvtabular for this work. It offers optimized tabular data preprocessing operators that run on the GPU. If you would like to learn more about this software library, please take a look [here](https://github.com/NVIDIA-Merlin/NVTabular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvtabular import *\n",
    "from nvtabular import ops\n",
    "from merlin.models.tf import Loader\n",
    "\n",
    "from merlin.schema.tags import Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3435af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_dataset = Dataset(train_set)\n",
    "validation_set_dataset = Dataset(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bd5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_features = ['city_id', 'booker_country', 'utrip_id', 'checkin'] >> ops.Groupby(\n",
    "    groupby_cols=['utrip_id'],\n",
    "    sort_cols=['checkin'],\n",
    "    aggs={\n",
    "        'city_id': ['list', 'count'],\n",
    "        'booker_country': ['list']\n",
    "    }\n",
    ")\n",
    "\n",
    "groupby_features_truncated_city = groupby_features['city_id_list'] >> ops.Categorify() >> ops.ListSlice(0, MAX_TRIP_LENGTH, pad=True) >> ops.AddTags([Tags.SEQUENCE, Tags.ITEM, Tags.ITEM_ID])\n",
    "groupby_features_truncated_country = groupby_features['booker_country_list'] >> ops.Categorify() >> ops.ListSlice(0, MAX_TRIP_LENGTH, pad=True) >> ops.AddTags([Tags.SEQUENCE, Tags.ITEM])\n",
    "city_id_count = groupby_features['city_id_count'] >> ops.AddTags([Tags.CONTEXT, Tags.ITEM, Tags.CONTINUOUS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = Workflow(groupby_features_truncated_city + groupby_features_truncated_country + city_id_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcbcdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed = wf.fit_transform(train_set_dataset)\n",
    "validation_set_processed = wf.fit_transform(validation_set_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a6675",
   "metadata": {},
   "source": [
    "Our data consists of a sequence of visited `city_ids`, a sequence of `booker_countries` (represented as integer categories) and a `city_id_count` column (which contains the count of visited cities in a trip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed.compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89cc3a0",
   "metadata": {},
   "source": [
    "We are now ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import merlin.models.tf as mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95c794",
   "metadata": {},
   "source": [
    "Let's identify two schemas. The first one for sequential features, the other for context features (`city_id_count`) that we will broadcast to the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4813456",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_schema = train_set_processed.schema.select_by_tag(Tags.SEQUENCE)\n",
    "context_schema = train_set_processed.schema.select_by_tag(Tags.CONTEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d422833",
   "metadata": {},
   "source": [
    "Let's also identify the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a3a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_set_processed.schema.select_by_tag(Tags.SEQUENCE).column_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed.schema.select_by_tag(Tags.SEQUENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12009aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_processed.schema.select_by_tag(Tags.CONTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddfd424",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader(train_set_processed, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcbcef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.models.tf.transforms.features import BroadcastToSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40e5c0",
   "metadata": {},
   "source": [
    "## Without broadcasting of context features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.Model(\n",
    "    mm.InputBlockV2(\n",
    "        seq_schema,\n",
    "        embeddings=mm.Embeddings(\n",
    "            train_set_processed.schema.select_by_tag(Tags.CATEGORICAL), sequence_combiner=None\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    mm.GPT2Block(d_model=40, n_head=4, n_layer=2, pre=mm.ReplaceMaskedEmbeddings()),\n",
    "    mm.CategoricalOutput(\n",
    "        train_set_processed.schema.select_by_name(target),\n",
    "        default_loss=\"categorical_crossentropy\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c8e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(run_eagerly=False, optimizer='adam', loss=\"categorical_crossentropy\")\n",
    "model.fit(loader, pre=mm.SequenceMaskRandom(schema=seq_schema, target=target, masking_prob=0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b1bc6",
   "metadata": {},
   "source": [
    "## Specifying correct input_dimensions (before broadcasting) in model constructor (d_model=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf216b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.Model(\n",
    "    mm.InputBlockV2(\n",
    "        train_set_processed.schema,\n",
    "        embeddings=mm.Embeddings(\n",
    "            train_set_processed.schema.select_by_tag(Tags.CATEGORICAL), sequence_combiner=None\n",
    "        ),\n",
    "        post=BroadcastToSequence(context_schema, seq_schema)\n",
    "    ),\n",
    "\n",
    "    mm.GPT2Block(d_model=40, n_head=4, n_layer=2, pre=mm.ReplaceMaskedEmbeddings()),\n",
    "    mm.CategoricalOutput(\n",
    "        train_set_processed.schema.select_by_name(target),\n",
    "        default_loss=\"categorical_crossentropy\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f16fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(run_eagerly=True, optimizer='adam', loss=\"categorical_crossentropy\")\n",
    "model.fit(loader, pre=mm.SequenceMaskRandom(schema=train_set_processed.schema, target=target, masking_prob=0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a8e13f",
   "metadata": {},
   "source": [
    "## Specifying correct input_dimensions (after broadcasting) in model constructor (d_model=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.Model(\n",
    "    mm.InputBlockV2(\n",
    "\n",
    "        train_set_processed.schema,\n",
    "        embeddings=mm.Embeddings(\n",
    "            train_set_processed.schema.select_by_tag(Tags.CATEGORICAL), sequence_combiner=None\n",
    "        ),\n",
    "        post=BroadcastToSequence(context_schema, seq_schema)\n",
    "    ),\n",
    "\n",
    "    mm.GPT2Block(d_model=41, n_head=4, n_layer=2, pre=mm.ReplaceMaskedEmbeddings()),\n",
    "    mm.CategoricalOutput(\n",
    "        train_set_processed.schema.select_by_name(target),\n",
    "        default_loss=\"categorical_crossentropy\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00160990",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(run_eagerly=True, optimizer='adam', loss=\"categorical_crossentropy\")\n",
    "model.fit(loader, pre=mm.SequenceMaskRandom(schema=train_set_processed.schema, target=target, masking_prob=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c662af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first stab at evaluation\n",
    "\n",
    "loader_eval = Loader(validation_set_processed, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(loader_eval, batch_size=1024, pre=mm.SequenceMaskLast(schema=train_set_processed.schema, target=target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
