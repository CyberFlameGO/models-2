{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef5df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854815b0",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_models_ecommerce-session-based-next-item-prediction-for-fashion/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Session-Based Next Item Prediction for Fashion E-Commerce\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container. \n",
    "\n",
    "## Overview\n",
    "\n",
    "NVIDIA-Merlin team participated in [Recsys2022 challenge](http://www.recsyschallenge.com/2022/index.html) and secured 3rd position. This notebook contains the various techniques used in the solution.\n",
    "\n",
    "### Learning Objective\n",
    "\n",
    "In this notebook, we will apply important concepts that improve recommender systems. We leveraged them for our RecSys solution:\n",
    "- MultiClass next item prediction head with Merlin Models\n",
    "- Sequential input features representing user sessions\n",
    "- Label Smoothing \n",
    "- Temperature Scaling\n",
    "- Weight Tying\n",
    "- Learning Rate Scheduler\n",
    "\n",
    "### Brief Description of the Concepts\n",
    "\n",
    "##### Label smoothing\n",
    "In recommender systems, we often have noisy datasets. A user cannot view all items to make the best decision. Noisy examples can result in high gradients and confuse the model. Label smoothing addresses the problem of noisy examples by smoothing the porbabilities to avoid high confident predictions.\n",
    "\n",
    "$$  \\begin{array}{l}\n",
    "y_{l} \\ =\\ ( 1\\ -\\ \\alpha \\ ) \\ *\\ y_{o} \\ +\\ ( \\alpha \\ /\\ L)\\\\\n",
    "\\alpha :\\ Label\\ smoothing\\ hyper-parameter\\ ( 0 \\leq \\alpha \\leq 1 ) \\\\\n",
    "L:\\ Total\\ number\\ of\\ label\\ classes\\\\\n",
    "y_{o} :\\ One-hot\\ encoded\\ label\\ vector\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "When α is 0, we have the original one-hot encoded labels, and as α increases, we move towards smoothed labels. Read [this](https://arxiv.org/abs/1906.02629) paper to learn more about it.\n",
    "\n",
    "\n",
    "##### Temperature Scaling\n",
    "Similar to Label Smoothing, Temperature Scaling is done to reduce the overconfidence of a model. In this, we divide the logits (inputs to the softmax function) by a scalar parameter (T) . For more information on Temperature Scaling read [this](https://arxiv.org/pdf/1706.04599.pdf) paper.\n",
    "$$ softmax\\ =\\ \\frac{e\\ ^{( z_{i} \\ /\\ \\ T)}}{\\sum _{j} \\ e^{( z_{j} \\ /\\ T)} \\ } $$\n",
    "\n",
    "\n",
    "##### Weight Tying\n",
    "Weight Tying can be applied for Multi-Class Classification problems, when we try to predict items and have previous viewed items as an input. The final output layer (without activation function) is multiplied with the traversed item embeddings, resulting in a vector with a logit for each item id. The advantage is that the gradients flow to the item embeddings are short. For more information read [this](https://arxiv.org/pdf/1608.05859v3.pdf) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af678b4c",
   "metadata": {},
   "source": [
    "## Downloading and preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee802a",
   "metadata": {},
   "source": [
    "We will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0e1975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2022-11-09 16:24:12.266810: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-09 16:24:14.428503: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0\n",
      "2022-11-09 16:24:14.428599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16254 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:15:00.0, compute capability: 7.0\n",
      "2022-11-09 16:24:14.429270: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 1\n",
      "2022-11-09 16:24:14.429331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 29671 MB memory:  -> device: 1, name: Quadro GV100, pci bus id: 0000:2d:00.0, compute capability: 7.0\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "import glob\n",
    "\n",
    "import nvtabular as nvt\n",
    "from merlin.io import Dataset\n",
    "from merlin.schema import Schema, Tags\n",
    "from nvtabular.ops import (\n",
    "    AddMetadata,\n",
    ")\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "from merlin.models.tf import InputBlock\n",
    "from merlin.models.tf.models.base import Model\n",
    "from merlin.models.tf.transforms.bias import LogitsTemperatureScaler\n",
    "from merlin.models.tf.prediction_tasks.next_item import ItemsPredictionWeightTying\n",
    "\n",
    "from merlin.core.dispatch import get_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92ec55",
   "metadata": {},
   "source": [
    "###  Dressipi\n",
    "[Dressipi](http://www.recsyschallenge.com/2022/dataset.html) hosted the [Recsys2022 challenge](http://www.recsyschallenge.com/2022/index.html) and provided an anonymized dataset. It contains 1.1 M online retail sessions that resulted in a purchase. It provides details about items that were viewed in a session, the item purchased at the end of the session and numerous features of those items. The item features are categorical IDs and are not interpretable.\n",
    "\n",
    "The task of this competition was, given a sequence of items predict which item will be purchased at the end of a session.\n",
    "\n",
    "<img src=\"http://www.recsyschallenge.com/2022/images/session_purchase_data.jpeg\" alt=\"dressipi_dataset\" style=\"width: 400px; float: center;\">  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afab83f",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We provide a function `get_dressipi2022` which preprocess the dataset. Currently, we can't download this dataset automatically so this needs to be downloaded manually. To use this function, prepare the data by following these 3 steps:\n",
    "1. Sign up and download the data from [dressipi-recsys2022.com](https://www.dressipi-recsys2022.com/).\n",
    "2. Unzip the raw data to a directory.\n",
    "3. Define `DATA_FOLDER` to the directory\n",
    "\n",
    "In case you do not want to use this dataset to run our examples, you can also opt for synthetic data. Synthetic data can be generated by running::\n",
    "\n",
    "```python\n",
    "    from merlin.datasets.synthetic import generate_data\n",
    "    train, valid = generate_data(\"dressipi2022-preprocessed\", num_rows=10000, set_sizes=(0.8, 0.2))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd734f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.datasets.ecommerce import get_dressipi2022\n",
    "\n",
    "DATA_FOLDER = os.environ.get(\n",
    "    \"DATA_FOLDER\", \n",
    "    '/workspace/data/dressipi_recsys2022'\n",
    ")\n",
    "\n",
    "train, valid = get_dressipi2022(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb6edf8",
   "metadata": {},
   "source": [
    "The dataset contains:\n",
    "- `session_id`, id of a session, in which a user viewed and purchased an item. \n",
    "- `item_id` which was viewed at a given `timestamp` in a session\n",
    "- `purchase_id` which is the id of item bought at the end of the session \n",
    "\n",
    "In addition to `timestamp`, we have `day` and `date` features for representing the chronological order in which items were viewed.\n",
    "\n",
    "The items in the Dresspi dataset had a many features out of which we took 22 most important features, namely \n",
    "`f_3 ,f_4 ,f_5 ,f_7 ,f_17 ,f_24 ,f_30 ,f_45 ,f_46 ,f_47 ,f_50 ,f_53 ,f_55 ,f_56 ,f_58 ,f_61 ,f_63 ,f_65 ,f_68 ,f_69 ,f_72 ,f_73`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e362d59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_17</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_45</th>\n",
       "      <th>f_47</th>\n",
       "      <th>...</th>\n",
       "      <th>f_61</th>\n",
       "      <th>f_63</th>\n",
       "      <th>f_65</th>\n",
       "      <th>f_68</th>\n",
       "      <th>f_69</th>\n",
       "      <th>f_72</th>\n",
       "      <th>f_73</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>day</th>\n",
       "      <th>purchase_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13441</td>\n",
       "      <td>19420</td>\n",
       "      <td>2020-08-14 18:42:59.970</td>\n",
       "      <td>793</td>\n",
       "      <td>605</td>\n",
       "      <td>536</td>\n",
       "      <td>378</td>\n",
       "      <td>588</td>\n",
       "      <td>177</td>\n",
       "      <td>242</td>\n",
       "      <td>...</td>\n",
       "      <td>808</td>\n",
       "      <td>816</td>\n",
       "      <td>579</td>\n",
       "      <td>739</td>\n",
       "      <td>805</td>\n",
       "      <td>75</td>\n",
       "      <td>-1</td>\n",
       "      <td>1597430579970</td>\n",
       "      <td>226</td>\n",
       "      <td>23039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13441</td>\n",
       "      <td>22734</td>\n",
       "      <td>2020-08-14 18:43:17.426</td>\n",
       "      <td>793</td>\n",
       "      <td>605</td>\n",
       "      <td>536</td>\n",
       "      <td>378</td>\n",
       "      <td>-1</td>\n",
       "      <td>559</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>706</td>\n",
       "      <td>861</td>\n",
       "      <td>521</td>\n",
       "      <td>373</td>\n",
       "      <td>780</td>\n",
       "      <td>75</td>\n",
       "      <td>544</td>\n",
       "      <td>1597430597426</td>\n",
       "      <td>226</td>\n",
       "      <td>23039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13441</td>\n",
       "      <td>13369</td>\n",
       "      <td>2020-08-14 18:44:53.299</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>394</td>\n",
       "      <td>-1</td>\n",
       "      <td>588</td>\n",
       "      <td>-1</td>\n",
       "      <td>123</td>\n",
       "      <td>...</td>\n",
       "      <td>706</td>\n",
       "      <td>861</td>\n",
       "      <td>-1</td>\n",
       "      <td>373</td>\n",
       "      <td>805</td>\n",
       "      <td>75</td>\n",
       "      <td>544</td>\n",
       "      <td>1597430693299</td>\n",
       "      <td>226</td>\n",
       "      <td>23039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13441</td>\n",
       "      <td>23304</td>\n",
       "      <td>2020-08-14 18:45:33.083</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>452</td>\n",
       "      <td>378</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>165</td>\n",
       "      <td>...</td>\n",
       "      <td>706</td>\n",
       "      <td>861</td>\n",
       "      <td>521</td>\n",
       "      <td>393</td>\n",
       "      <td>592</td>\n",
       "      <td>75</td>\n",
       "      <td>-1</td>\n",
       "      <td>1597430733083</td>\n",
       "      <td>226</td>\n",
       "      <td>23039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13441</td>\n",
       "      <td>19653</td>\n",
       "      <td>2020-08-14 18:45:47.108</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>536</td>\n",
       "      <td>378</td>\n",
       "      <td>588</td>\n",
       "      <td>-1</td>\n",
       "      <td>516</td>\n",
       "      <td>...</td>\n",
       "      <td>462</td>\n",
       "      <td>861</td>\n",
       "      <td>521</td>\n",
       "      <td>379</td>\n",
       "      <td>805</td>\n",
       "      <td>75</td>\n",
       "      <td>544</td>\n",
       "      <td>1597430747108</td>\n",
       "      <td>226</td>\n",
       "      <td>23039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id  item_id                    date  f_3  f_5  f_7  f_17  f_24  \\\n",
       "0       13441    19420 2020-08-14 18:42:59.970  793  605  536   378   588   \n",
       "1       13441    22734 2020-08-14 18:43:17.426  793  605  536   378    -1   \n",
       "2       13441    13369 2020-08-14 18:44:53.299   -1   -1  394    -1   588   \n",
       "3       13441    23304 2020-08-14 18:45:33.083   -1   -1  452   378    -1   \n",
       "4       13441    19653 2020-08-14 18:45:47.108   -1   -1  536   378   588   \n",
       "\n",
       "   f_45  f_47  ...  f_61  f_63  f_65  f_68  f_69  f_72  f_73      timestamp  \\\n",
       "0   177   242  ...   808   816   579   739   805    75    -1  1597430579970   \n",
       "1   559    36  ...   706   861   521   373   780    75   544  1597430597426   \n",
       "2    -1   123  ...   706   861    -1   373   805    75   544  1597430693299   \n",
       "3    -1   165  ...   706   861   521   393   592    75    -1  1597430733083   \n",
       "4    -1   516  ...   462   861   521   379   805    75   544  1597430747108   \n",
       "\n",
       "   day  purchase_id  \n",
       "0  226        23039  \n",
       "1  226        23039  \n",
       "2  226        23039  \n",
       "3  226        23039  \n",
       "4  226        23039  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.to_ddf().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a4235e",
   "metadata": {},
   "source": [
    "## Feature Engineering with NVTabular\n",
    "\n",
    "We use NVTabular for Feature Engineering. If you want to learn more about NVTabular, we recommend the [examples in the NVTabular GitHub Repository](https://github.com/NVIDIA-Merlin/NVTabular/tree/main/examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5e2b7",
   "metadata": {},
   "source": [
    "### Categorify\n",
    "\n",
    "We want to use embedding layers for our categorical features. First, we need to Categorify them, that they are contiguous integers. \n",
    "\n",
    "The features `item_id` and `purchase_id` belongs to the same category. If `item_id` is 8432 and `purchase_id` is 8432, they are the same item. When we want to apply Categorify, we want to keep the connection. We can achieve this by encoding them jointly by providing them as a list in the list `[['item_id', 'purchase_id']]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389a708",
   "metadata": {},
   "source": [
    "We will use only 2 of the categorical item features in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f834311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80 µs, sys: 29 µs, total: 109 µs\n",
      "Wall time: 112 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "item_features_names = ['f_' + str(col) for col in [47, 68]]\n",
    "cat_features = [['item_id', 'purchase_id']] + item_features_names >> nvt.ops.Categorify(start_index=1, dtype='int32')\n",
    "\n",
    "features = ['session_id', 'timestamp', 'date'] + cat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe10a791",
   "metadata": {},
   "source": [
    "### GroupBy the data by sessions.\n",
    "\n",
    "Currently, every row is a viewed item in the dataset. Our goal is to predict the item purchased after the last view in a session. Therefore, we groupby the dataset by `session_id` to have one row for each prediction.\n",
    "\n",
    "Each row will have a sequence of encoded items ids with which a user interacted. The last item of a session has special importance as it is closer to the user's intention. We will keep the viewed item as a separate feature.\n",
    "\n",
    "The NVTabular `GroupBy` op enables the transformation. \n",
    "\n",
    "First, we define how the different columns should be aggregates:\n",
    "- Keep the first occurrence of `date`\n",
    "- Keep the last item and concatenate all items to a list (results are 2 features)\n",
    "- Keep the first occurrence of `purchase_id` (purchase_id should be the same for all rows of one session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de40cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_aggregate = {}\n",
    "to_aggregate['date'] = [\"first\"]\n",
    "to_aggregate['item_id'] = [\"last\", \"list\"]\n",
    "to_aggregate['purchase_id'] = [\"first\"]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453abf34",
   "metadata": {},
   "source": [
    "In addition, we concatenate each item features to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d6450e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in item_features_names: \n",
    "    to_aggregate[name] = ['list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6963a0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': ['first'],\n",
       " 'item_id': ['last', 'list'],\n",
       " 'purchase_id': ['first'],\n",
       " 'f_47': ['list'],\n",
       " 'f_68': ['list']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caead45e",
   "metadata": {},
   "source": [
    "We want to sort the dataframe by `date` and groupby the columns by `session_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad84c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_features = features >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"session_id\"], \n",
    "    sort_cols=[\"date\"],\n",
    "    aggs= to_aggregate,\n",
    "    name_sep=\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375e1d5",
   "metadata": {},
   "source": [
    "Merlin Models can infer the neural network architecture from the dataset schema. We will Tag the columns accordingly based on the type of each column. If you want to learn more, we recommend our [Dataset Schema Example](https://github.com/NVIDIA-Merlin/models/blob/main/examples/02-Merlin-Models-and-NVTabular-integration.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8732e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_last = (\n",
    "    groupby_features['item_id_last'] >> \n",
    "    AddMetadata(tags=[Tags.ITEM, Tags.ITEM_ID])\n",
    ")\n",
    "item_list = (\n",
    "    groupby_features['item_id_list'] >> \n",
    "    AddMetadata(\n",
    "        tags=[Tags.ITEM, Tags.ITEM_ID, Tags.LIST, Tags.SEQUENCE]\n",
    "    )\n",
    ")\n",
    "feature_list = (\n",
    "    groupby_features[[name+'_list' for name in item_features_names]] >> \n",
    "    AddMetadata(\n",
    "        tags=[Tags.SEQUENCE, Tags.ITEM, Tags.LIST]\n",
    "    )\n",
    ")\n",
    "target_feature = (\n",
    "    groupby_features['purchase_id_first'] >> \n",
    "    AddMetadata(tags=[Tags.TARGET])\n",
    ")\n",
    "other_features = groupby_features['session_id', 'date_first']\n",
    "\n",
    "groupby_features = item_last + item_list + feature_list + other_features +  groupby_features['purchase_id_first']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9a9832",
   "metadata": {},
   "source": [
    "### Truncate for a Maximum Sequence Length\n",
    "\n",
    "We want to truncate and pad the sequential features. We define the columns, which are sequential features and the non-sequential ones. We truncate the sequence by keeping the last 3 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e65dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features = [name+'_list' for name in item_features_names] + ['item_id_list']\n",
    "nonlist_features = ['session_id', 'date_first', 'item_id_last', 'purchase_id_first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0d338d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSIONS_MAX_LENGTH = 3\n",
    "truncated_features = groupby_features[list_features] >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH) >> nvt.ops.Rename(postfix = '_seq')\n",
    "\n",
    "final_features = groupby_features[nonlist_features] + truncated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2caa1",
   "metadata": {},
   "source": [
    "We initialize our NVTabular workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74b030ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = nvt.Workflow(final_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6954007",
   "metadata": {},
   "source": [
    "We call fit and transform similar to the scikit learn API.\n",
    "\n",
    "Categorify will map item_ids (and purchase_ids), which does not occur in the train dataset, to a special category `0` in the validation dataset. This can bias the validation metrics. In our example, almost all item_ids in validation are available in train and we neglect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65f5c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit data\n",
    "workflow.fit(train)\n",
    "\n",
    "# transform and save data\n",
    "workflow.transform(train).to_parquet(os.path.join(DATA_FOLDER, \"train/\"), output_files=2)\n",
    "workflow.transform(valid).to_parquet(os.path.join(DATA_FOLDER, \"valid/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d222c0e",
   "metadata": {},
   "source": [
    "### Sort the Training Dataset by Time\n",
    "\n",
    "The train dataset contains the data from Jan 2020 to April 2021 and the validation dataset is May 2021. As the data is split by time, we noticed that we achieve higher validation scores, when we sort the training data by time and do not apply shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ba81642",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_lib().read_parquet(\n",
    "    glob.glob(\n",
    "        os.path.join(DATA_FOLDER, \"train/*.parquet\")\n",
    "    )\n",
    ")\n",
    "df = df.sort_values('date_first').reset_index(drop=True)\n",
    "df.to_parquet(os.path.join(DATA_FOLDER, \"train_sorted.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c04dc",
   "metadata": {},
   "source": [
    "Let's review the transformed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03de5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04948a31",
   "metadata": {},
   "source": [
    "## Training an MLP with sequential input with Merlin Models\n",
    "\n",
    "We train a Sequential-Multi-Layer Perceptron model, which averages the sequential input features (e.g. `item_id_list_seq`) and concatenate the resulting embeddings with the categorical embeddings (e.g. `item_id_last`). We visualize the architecture in the figure below.\n",
    "\n",
    "<img src=\"../images/mlp_ecommerce.png\"  width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d6a7d",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "We initialize the dataloaders to train the neural network models. First, we define NVTabular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f673e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:148: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train = Dataset(os.path.join(DATA_FOLDER, 'train_sorted.parquet'))\n",
    "valid = Dataset(os.path.join(DATA_FOLDER, 'valid/*.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5546df60",
   "metadata": {},
   "source": [
    "As we loaded, sorted and saved the train dataset without using NVTabular, the parquet file doesn't contain a schema, anymore. We can copy the schema from valid to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f89a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.schema = valid.schema\n",
    "schema_model = train.schema.select_by_name(\n",
    "        ['item_id_list_seq', 'item_id_last','f_47_list_seq', 'f_68_list_seq']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92782bbb-5715-49b6-9750-fda93fb36a5d",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "We use the following hyperparameters, we found during experimentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45d40de0-138c-4410-83df-cda5a34867c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = int(os.environ.get(\n",
    "    \"EPOCHS\", \n",
    "    '5'\n",
    "))\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.01\n",
    "DROPOUT = 0.2 \n",
    "LABEL_SMOOTHING = 0.2\n",
    "TEMPERATURE_SCALING = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e197311",
   "metadata": {},
   "source": [
    "### Build the Sequential MLP with Merlin Models\n",
    "\n",
    "Now we will create an InputBlock which takes sequential features, concatenate them and return the sequence of interaction embeddings. Note that we define the embedding dimensions, manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4dce190",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_dims = {\n",
    "    'item_id_list_seq': 256, \n",
    "    'item_id_last': 256,\n",
    "    'f_47_list_seq': 16,\n",
    "    'f_68_list_seq': 16\n",
    "}\n",
    "\n",
    "input_block = mm.InputBlockV2(\n",
    "        schema_model,\n",
    "        categorical=mm.Embeddings(schema_model,\n",
    "                                 dim=manual_dims\n",
    "                                )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5241379",
   "metadata": {},
   "source": [
    "Before the loss is calculated, we want to transform the model output:\n",
    "1. We apply L2 norm to the model logits.\n",
    "2. We apply `weight-tying` and multiply the model output with the embedding weights from ITEM_ID.  The embedding dimensions and the model output dimensions have to be the same (256 in our example).\n",
    "3. We transform the ground truth into OneHot representation\n",
    "4. We apply Temperature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85994ba-49d7-4630-b1c8-c4cc520b49e7",
   "metadata": {},
   "source": [
    "Now, we will build a model with a 2-layer MLPBlock, `input_block` as the input and `prediction_task` as the task. The output dimension of MLPBlock should match with the embedding dimension of the `item_id_list_seq` since we are using weight tying technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3abeac63-876a-428a-bfaa-72bcba966646",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block = mm.MLPBlock(\n",
    "        [128, 256], \n",
    "        no_activation_last_layer=True, \n",
    "        dropout=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75d6a85d-1d7e-4158-ade6-ba1386f001bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'item_id_purchase_id'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_id_name = train.schema.select_by_tag(Tags.ITEM_ID).first.properties['domain']['name']\n",
    "item_id_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5cbe51-a5cb-4964-9bb2-7ecf53ee36ec",
   "metadata": {},
   "source": [
    "Next, we define the prediction task. Our objective is multi-class classification - which is the item purchased at the end of the session. Therefore, this is a multi-class classification task, and the default_loss in the `CategoricalOutput` class is  \"categorical_crossentropy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b053041",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_task= mm.CategoricalOutput(\n",
    "        to_call=input_block[\"categorical\"][item_id_name],\n",
    "        logits_temperature=TEMPERATURE_SCALING,\n",
    "        target='purchase_id_first',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76f47160-f2df-4e6b-bff2-4b8b3e9bef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp = mm.Model(input_block, mlp_block, prediction_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54efec72",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c674e-3b81-4ba1-893d-e320d9810ddf",
   "metadata": {},
   "source": [
    "The default dataloader does shuffle by default. We will initialize the Loader for the training dataset, and set the shuffle to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "992211b3-f647-460b-989b-181bce05b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = mm.Loader(train, batch_size=BATCH_SIZE, transform=mm.ToTarget(train.schema, \"purchase_id_first\", one_hot=True),  shuffle = False)\n",
    "val_loader = mm.Loader(valid, batch_size=BATCH_SIZE, transform=mm.ToTarget(train.schema, \"purchase_id_first\", one_hot=True),  shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51be5866-f7f3-4111-b461-7b23eea7d7c2",
   "metadata": {},
   "source": [
    "\n",
    "We initialize the optimizer with `ExponentialDecay` learning rate scheduler and compile the model - similar to other TensorFlow Keras API. The competition was evaluated based on MRR@100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "203030aa-71d5-4167-b96c-96e99b6d9c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = LEARNING_RATE\n",
    "\n",
    "exp_decay_lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cd66c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=exp_decay_lr_scheduler,\n",
    ")\n",
    "\n",
    "model_mlp.compile(\n",
    "    optimizer=optimizer,\n",
    "    run_eagerly=True,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, \n",
    "        label_smoothing=LABEL_SMOOTHING\n",
    "    ),\n",
    "    metrics=mm.TopKMetricsAggregator.default_metrics(top_ks=[100])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e82bee",
   "metadata": {},
   "source": [
    "We call `.fit` to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f6deb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "900/900 [==============================] - 193s 209ms/step - loss: 7.9981 - recall_at_100: 0.3590 - mrr_at_100: 0.0753 - ndcg_at_100: 0.1284 - map_at_100: 0.0753 - precision_at_100: 0.0036 - regularization_loss: 0.0000e+00 - loss_batch: 7.9972 - val_loss: 7.9847 - val_recall_at_100: 0.4734 - val_mrr_at_100: 0.0954 - val_ndcg_at_100: 0.1675 - val_map_at_100: 0.0954 - val_precision_at_100: 0.0047 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 7.7934\n",
      "Epoch 2/5\n",
      "900/900 [==============================] - 188s 207ms/step - loss: 7.6937 - recall_at_100: 0.4454 - mrr_at_100: 0.0960 - ndcg_at_100: 0.1622 - map_at_100: 0.0960 - precision_at_100: 0.0045 - regularization_loss: 0.0000e+00 - loss_batch: 7.6924 - val_loss: 8.0149 - val_recall_at_100: 0.4625 - val_mrr_at_100: 0.0888 - val_ndcg_at_100: 0.1595 - val_map_at_100: 0.0888 - val_precision_at_100: 0.0046 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 7.8448\n",
      "Epoch 3/5\n",
      "900/900 [==============================] - 190s 209ms/step - loss: 7.6130 - recall_at_100: 0.4778 - mrr_at_100: 0.1028 - ndcg_at_100: 0.1740 - map_at_100: 0.1028 - precision_at_100: 0.0048 - regularization_loss: 0.0000e+00 - loss_batch: 7.6112 - val_loss: 8.0830 - val_recall_at_100: 0.4523 - val_mrr_at_100: 0.0915 - val_ndcg_at_100: 0.1598 - val_map_at_100: 0.0915 - val_precision_at_100: 0.0045 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 7.9777\n",
      "Epoch 4/5\n",
      "900/900 [==============================] - 187s 207ms/step - loss: 7.5536 - recall_at_100: 0.4967 - mrr_at_100: 0.1081 - ndcg_at_100: 0.1819 - map_at_100: 0.1081 - precision_at_100: 0.0050 - regularization_loss: 0.0000e+00 - loss_batch: 7.5515 - val_loss: 8.1354 - val_recall_at_100: 0.4441 - val_mrr_at_100: 0.0914 - val_ndcg_at_100: 0.1583 - val_map_at_100: 0.0914 - val_precision_at_100: 0.0044 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 8.0490\n",
      "Epoch 5/5\n",
      "900/900 [==============================] - 188s 207ms/step - loss: 7.5191 - recall_at_100: 0.5078 - mrr_at_100: 0.1103 - ndcg_at_100: 0.1859 - map_at_100: 0.1103 - precision_at_100: 0.0051 - regularization_loss: 0.0000e+00 - loss_batch: 7.5168 - val_loss: 8.1689 - val_recall_at_100: 0.4458 - val_mrr_at_100: 0.0908 - val_ndcg_at_100: 0.1577 - val_map_at_100: 0.0908 - val_precision_at_100: 0.0045 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 8.0466\n",
      "CPU times: user 12min 57s, sys: 3min 55s, total: 16min 52s\n",
      "Wall time: 15min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model_mlp.fit(\n",
    "    loader,\n",
    "    validation_data=val_loader,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160f023",
   "metadata": {},
   "source": [
    "We can evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f114178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 20s 257ms/step - loss: 8.1689 - recall_at_100: 0.4466 - mrr_at_100: 0.0902 - ndcg_at_100: 0.1573 - map_at_100: 0.0902 - precision_at_100: 0.0045 - regularization_loss: 0.0000e+00 - loss_batch: 8.1663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09081850945949554"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_mlp = model_mlp.evaluate(val_loader, batch_size=BATCH_SIZE, return_dict=True)\n",
    "metrics_mlp['mrr_at_100']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8dbe8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training a Bi-LSTM with Merlin Models\n",
    "\n",
    "\n",
    "In this section, we train a Bi-LSTM model, which concatenates the embedding vectors for all sequential features (`item_id_list_seq`, `f_47_list_seq`, `f_68_list_seq`) per step (e.g. here 3). The concatenated vectors are processed by a BiLSTM. The hidden state of the BiLSTM is concatenated with the embedding vectors of the categorical features (`item_id_last`). Then we connect it with a Multi-Layer Perceptron Block. We visualize the architecture in the figure below.\n",
    "\n",
    "<img src=\"../images/bi-lstm_ecommerce.png\"  width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081cf607",
   "metadata": {},
   "source": [
    "### Build the Sequential MLP with Merlin Models\n",
    "Now we will create two InputBlock which takes sequential and categorical features, concatenate them and return the interaction embeddings.\n",
    "\n",
    "#### Hyperparameters\n",
    "We only updated the `LEARNING_RATE`, as in our experimentations we found Bi-LSTM gave better results with lower learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d74c292-22c9-4870-b225-3d41e7526152",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70108db8-235a-496f-9787-f41d6193318d",
   "metadata": {},
   "source": [
    "We define the embedding dimensions, manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9275b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_inputs = mm.InputBlockV2(\n",
    "        schema_model.select_by_name(\n",
    "            ['item_id_list_seq', 'f_47_list_seq', 'f_68_list_seq']\n",
    "        ),\n",
    "        categorical=mm.Embeddings(\n",
    "            schema_model.select_by_name(\n",
    "            ['item_id_list_seq', 'f_47_list_seq', 'f_68_list_seq']\n",
    "        ),\n",
    "            dim=manual_dims\n",
    "                                \n",
    "        )\n",
    ")\n",
    "\n",
    "cat_inputs = mm.InputBlockV2(\n",
    "        schema_model.select_by_name(\n",
    "            ['item_id_last']\n",
    "        ),\n",
    "        categorical=mm.Embeddings(\n",
    "            schema_model.select_by_name(\n",
    "            ['item_id_last']\n",
    "        ),\n",
    "            dim=manual_dims\n",
    "                                \n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5672fae7",
   "metadata": {},
   "source": [
    "### Build Bi-LSTM model\n",
    "\n",
    "Now we will create a Bi-LSTM model by defining a custom layer of type mm.Block. It will take sequential interaction embeddings as an input. We connect the sequence input block for sequential features with `Bi-LSTM` Block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33820698-9303-44bc-8798-b74e753e6a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_block =seq_inputs.connect(tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(64,\n",
    "        return_sequences=False, \n",
    "        dropout=0.05,\n",
    "        kernel_regularizer=regularizers.l2(1e-4),\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280055cb",
   "metadata": {},
   "source": [
    "Now, we combine `Bi-LSTM` block with `InputBlock` of categorical features by concatenating them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fde77f-b413-4cdb-bb6d-2cc2c61ae98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_block(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcd660b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "concats = mm.ParallelBlock(\n",
    "    {'dense_block': dense_block, \n",
    "     'cat_inputs': cat_inputs},\n",
    "    aggregation='concat'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4380ca59",
   "metadata": {},
   "source": [
    "Next, we will build a 2-layer MLPBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "deea812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block = mm.MLPBlock(\n",
    "                [128,256],\n",
    "                activation='relu',\n",
    "                no_activation_last_layer=True,\n",
    "                dropout=DROPOUT,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef2de27",
   "metadata": {},
   "source": [
    "Now, we define the prediction task. As we saw above in our MLP implementation, our objective is multi-class classification.\n",
    "\n",
    "Before the loss is calculated, we want to transform the model output:\n",
    "\n",
    "- We apply Weight Tying (see in the beginning) and multiply the model output with the embedding weights from ITEM_ID. The embedding dimensions and the model output dimensions have to be the same (256 in our example).\n",
    "- We transform the ground truth into OneHot representation\n",
    "- We apply Temperature Scaling\n",
    "\n",
    "Here we don't apply L2 norm to the model logits because it was detrimental to the Bi-LSTM model.\n",
    "\n",
    "Again same as above, the pipeline is called prediction_call and we add it as the pre parameter to MultiClassClassificationTask. The pipeline is executed before the loss is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fa8d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_task= mm.CategoricalOutput(\n",
    "        to_call=seq_inputs[\"categorical\"][item_id_name]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05106839",
   "metadata": {},
   "source": [
    "Now, we will build a model by chaining the `concats`, the `mlp_block` and the `prediction_task`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17e9c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi_lstm = Model(concats, mlp_block, prediction_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e6adf",
   "metadata": {},
   "source": [
    "### Fit the Model\n",
    "We initialize the optimizer and compile the model - similar to other TensorFlow Keras API. The competition was evaluated based on MRR@100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7673905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "\n",
    "model_bi_lstm.compile(\n",
    "    optimizer=optimizer,\n",
    "    run_eagerly=True,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, \n",
    "        label_smoothing=LABEL_SMOOTHING\n",
    "    ),\n",
    "    metrics=mm.TopKMetricsAggregator.default_metrics(top_ks=[100])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d845114e",
   "metadata": {},
   "source": [
    "Now, we train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7dfa4858-d87c-43db-a6fa-c02a55e83941",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"sequential_block_5\" (type SequentialBlock).\n\nInput 0 of layer \"bidirectional\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (1024, 288)\n\nCall arguments received by layer \"sequential_block_5\" (type SequentialBlock):\n  • inputs={'f_47_list_seq': ('tf.Tensor(shape=(2365, 1), dtype=int32)', 'tf.Tensor(shape=(1024, 1), dtype=int32)'), 'f_68_list_seq': ('tf.Tensor(shape=(2365, 1), dtype=int32)', 'tf.Tensor(shape=(1024, 1), dtype=int32)'), 'item_id_list_seq': ('tf.Tensor(shape=(2365, 1), dtype=int32)', 'tf.Tensor(shape=(1024, 1), dtype=int32)')}\n  • training=False\n  • kwargs={'features': {'f_47_list_seq': '<tf.RaggedTensor [[[3],\\n  [6],\\n  [14]], [[2],\\n          [2],\\n          [2]], [[14]], ..., [[4],\\n                              [4],\\n                              [4]], [[2],\\n                                     [2],\\n                                     [4]], [[14]]]>', 'f_68_list_seq': '<tf.RaggedTensor [[[2],\\n  [10],\\n  [8]] , [[10],\\n          [45],\\n          [7]] , [[9]], ..., [[28],\\n                              [23],\\n                              [2]] , [[28],\\n                                      [11],\\n                                      [2]] , [[10]]]>', 'item_id_list_seq': '<tf.RaggedTensor [[[538],\\n  [4177],\\n  [7920]], [[14809],\\n            [7840],\\n            [11594]], [[5192]], ..., [[11823],\\n                                      [19212],\\n                                      [6142]] , [[4396],\\n                                                 [11014],\\n                                                 [10622]], [[3855]]]>', 'item_id_last': 'tf.Tensor(shape=(1024, 1), dtype=int32)'}, 'testing': 'False'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_bi_lstm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/models/base.py:905\u001b[0m, in \u001b[0;36mBaseModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, train_metrics_steps, pre, **kwargs)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_compile_cache()\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_pre \u001b[38;5;241m=\u001b[39m pre\n\u001b[0;32m--> 905\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre:\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_pre\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/models/base.py:1185\u001b[0m, in \u001b[0;36mModel.call\u001b[0;34m(self, inputs, targets, training, testing, output_context)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     outputs, context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_child(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre, outputs, context)\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m-> 1185\u001b[0m     outputs, context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost:\n\u001b[1;32m   1188\u001b[0m     outputs, context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_child(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost, outputs, context)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/models/base.py:1214\u001b[0m, in \u001b[0;36mModel._call_child\u001b[0;34m(self, child, inputs, context)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(sub, ModelBlock) \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m child\u001b[38;5;241m.\u001b[39msubmodules):\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 1214\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, Prediction):\n\u001b[1;32m   1216\u001b[0m     targets \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;28;01mif\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m context\u001b[38;5;241m.\u001b[39mtargets\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/utils/tf_utils.py:437\u001b[0m, in \u001b[0;36mcall_layer\u001b[0;34m(layer, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m         call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m    435\u001b[0m         filtered_kwargs \u001b[38;5;241m=\u001b[39m filter_kwargs(filtered_kwargs, call_fn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_k)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfiltered_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/core/tabular.py:478\u001b[0m, in \u001b[0;36m_tabular_call\u001b[0;34m(self, inputs, pre, post, merge_with, aggregation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_call(inputs, transformations\u001b[38;5;241m=\u001b[39mpre)\n\u001b[1;32m    477\u001b[0m \u001b[38;5;66;03m# This will call the `call` method implemented by the super class.\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    481\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_call(\n\u001b[1;32m    482\u001b[0m         outputs, transformations\u001b[38;5;241m=\u001b[39mpost, merge_with\u001b[38;5;241m=\u001b[39mmerge_with, aggregation\u001b[38;5;241m=\u001b[39maggregation\n\u001b[1;32m    483\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/config/schema.py:58\u001b[0m, in \u001b[0;36mSchemaMixin.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_schema()\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/core/combinators.py:566\u001b[0m, in \u001b[0;36mParallelBlock.call\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    565\u001b[0m     layer_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_filter_layer_inputs_using_schema(name, layer, inputs)\n\u001b[0;32m--> 566\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcall_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    568\u001b[0m         out \u001b[38;5;241m=\u001b[39m {name: out}\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/utils/tf_utils.py:437\u001b[0m, in \u001b[0;36mcall_layer\u001b[0;34m(layer, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m         call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m    435\u001b[0m         filtered_kwargs \u001b[38;5;241m=\u001b[39m filter_kwargs(filtered_kwargs, call_fn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_k)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfiltered_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/config/schema.py:58\u001b[0m, in \u001b[0;36mSchemaMixin.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_schema()\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/core/combinators.py:269\u001b[0m, in \u001b[0;36mSequentialBlock.call\u001b[0;34m(self, inputs, training, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_sequentially\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/core/combinators.py:836\u001b[0m, in \u001b[0;36mcall_sequentially\u001b[0;34m(layers, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m outputs \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[0;32m--> 836\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/tf/utils/tf_utils.py:437\u001b[0m, in \u001b[0;36mcall_layer\u001b[0;34m(layer, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m         call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m    435\u001b[0m         filtered_kwargs \u001b[38;5;241m=\u001b[39m filter_kwargs(filtered_kwargs, call_fn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_k)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfiltered_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"sequential_block_5\" (type SequentialBlock).\n\nInput 0 of layer \"bidirectional\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (1024, 288)\n\nCall arguments received by layer \"sequential_block_5\" (type SequentialBlock):\n  • inputs={'f_47_list_seq': ('tf.Tensor(shape=(2365, 1), dtype=int32)', 'tf.Tensor(shape=(1024, 1), dtype=int32)'), 'f_68_list_seq': ('tf.Tensor(shape=(2365, 1), dtype=int32)', 'tf.Tensor(shape=(1024, 1), dtype=int32)'), 'item_id_list_seq': ('tf.Tensor(shape=(2365, 1), dtype=int32)', 'tf.Tensor(shape=(1024, 1), dtype=int32)')}\n  • training=False\n  • kwargs={'features': {'f_47_list_seq': '<tf.RaggedTensor [[[3],\\n  [6],\\n  [14]], [[2],\\n          [2],\\n          [2]], [[14]], ..., [[4],\\n                              [4],\\n                              [4]], [[2],\\n                                     [2],\\n                                     [4]], [[14]]]>', 'f_68_list_seq': '<tf.RaggedTensor [[[2],\\n  [10],\\n  [8]] , [[10],\\n          [45],\\n          [7]] , [[9]], ..., [[28],\\n                              [23],\\n                              [2]] , [[28],\\n                                      [11],\\n                                      [2]] , [[10]]]>', 'item_id_list_seq': '<tf.RaggedTensor [[[538],\\n  [4177],\\n  [7920]], [[14809],\\n            [7840],\\n            [11594]], [[5192]], ..., [[11823],\\n                                      [19212],\\n                                      [6142]] , [[4396],\\n                                                 [11014],\\n                                                 [10622]], [[3855]]]>', 'item_id_last': 'tf.Tensor(shape=(1024, 1), dtype=int32)'}, 'testing': 'False'}"
     ]
    }
   ],
   "source": [
    "history = model_bi_lstm.fit(\n",
    "    loader,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb684f",
   "metadata": {},
   "source": [
    "We can evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_bi_lstm = model_bi_lstm.evaluate(valid, batch_size=BATCH_SIZE, return_dict=True)\n",
    "metrics_bi_lstm['mrr_at_100']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c9a164-df68-417f-a634-c2966abe52a0",
   "metadata": {},
   "source": [
    "Note that final score `mrr_at_100` we printed out above is the average over all steps, whereas the value we get from progress bar shows the score of the last n_steps (by default =1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e525cdc3",
   "metadata": {},
   "source": [
    "## Training a Transformer-based Model\n",
    "\n",
    "In recent years, several deep learning-based algorithms have been proposed for recommendation systems while its adoption in industry deployments have been steeply growing. In particular, NLP inspired approaches have been successfully adapted for sequential and session-based recommendation problems, which are important for many domains like e-commerce, news and streaming media. Session-Based Recommender Systems (SBRS) have been proposed to model the sequence of interactions within the current user session, where a session is a short sequence of user interactions typically bounded by user inactivity. They have recently gained popularity due to their ability to capture short-term or contextual user preferences towards items.\n",
    "\n",
    "The field of NLP has evolved significantly within the last decade, particularly due to the increased usage of deep learning. As a result, state of the art NLP approaches have inspired RecSys practitioners and researchers to adapt those architectures, especially for sequential and session-based recommendation problems. Here, we use one of the state-of-the-art Transformer-based architecture, [XLNet](https://arxiv.org/abs/1906.08237) with Causal Language Modeling (CLM) training technique for multi-class classification task. For this, we leverage the popular HuggingFace’s Transformers NLP library and make it possible to experiment with cutting-edge implementation of such architectures for sequential and session-based recommendation problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deb8987-9ee7-4301-be2a-cc2efe2f9497",
   "metadata": {},
   "source": [
    "We train an `XLNet` model which concatenates the embedding vectors for all sequential features (`item_id_list_seq`, `f_47_list_seq`, `f_68_list_seq`) per step. Below we need an MLP projection due to residual connection in the Transformer model, therefore we add an MLPBlock in the model pipeline. `CategoricalOutput` has the functionality to do `weight-tying`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481d0f8-58aa-4191-ae96-30d1ddb82b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_name = train.schema.select_by_tag(Tags.ITEM_ID).first.properties['domain']['name']\n",
    "item_id_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19d1d918-fc3b-45f0-aee0-876b30897bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 32\n",
    "embeds = {'item_id_list_seq':EMBED_DIM,\n",
    "          'f_47_list_seq': 16,\n",
    "          'f_68_list_seq': 16\n",
    "         }\n",
    "\n",
    "inputs = mm.InputBlockV2(\n",
    "        train.schema.select_by_name(\n",
    "        ['item_id_list_seq', 'f_47_list_seq', 'f_68_list_seq']\n",
    "    ),\n",
    "        embeddings=mm.Embeddings(train.schema.select_by_name(['item_id_list_seq', 'f_47_list_seq', 'f_68_list_seq']\n",
    "                                                            ),\n",
    "                                 sequence_combiner=None,\n",
    "                                 dim=embeds\n",
    "                                )\n",
    ")\n",
    "\n",
    "model = mm.Model(\n",
    "    inputs,\n",
    "    mm.MLPBlock([EMBED_DIM]), \n",
    "    mm.XLNetBlock(\n",
    "        d_model=EMBED_DIM,\n",
    "        n_head=8,\n",
    "        n_layer=2,\n",
    "        post='sequence_mean',\n",
    "    ),\n",
    "    mm.CategoricalOutput(\n",
    "        to_call=inputs[\"categorical\"][item_id_name],\n",
    "        target=\"purchase_id_first\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b770cdb6-bc09-4273-9c9d-dd2a2b93b465",
   "metadata": {},
   "source": [
    "Define data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ec73bbb-deff-4dcc-834e-50a51675751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = mm.Loader(train, batch_size=BATCH_SIZE, transform=mm.ToTarget(train.schema, \"purchase_id_first\", one_hot=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc474897-1fe4-46cb-9c15-dd45f2ea7d22",
   "metadata": {},
   "source": [
    "We train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b2be155-a8ed-4a7b-bed4-6a9e7663d58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 16:32:40.424557: I tensorflow/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['model_1/embeddings:0', 'model_1/embeddings:0', 'model_1/mask_emb:0', 'transformer/layer_._0/rel_attn/r_s_bias:0', 'transformer/layer_._0/rel_attn/seg_embed:0', 'transformer/layer_._1/rel_attn/r_s_bias:0', 'transformer/layer_._1/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_1/xl_net_block/prepare_transformer_inputs_4/RaggedToTensor/boolean_mask_1/GatherV2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_1/xl_net_block/prepare_transformer_inputs_4/RaggedToTensor/boolean_mask/GatherV2:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_1/xl_net_block/prepare_transformer_inputs_4/RaggedToTensor/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['model_1/embeddings:0', 'model_1/embeddings:0', 'model_1/mask_emb:0', 'transformer/layer_._0/rel_attn/r_s_bias:0', 'transformer/layer_._0/rel_attn/seg_embed:0', 'transformer/layer_._1/rel_attn/r_s_bias:0', 'transformer/layer_._1/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "900/900 [==============================] - 123s 124ms/step - loss: 8.4775 - recall_at_100: 0.1522 - mrr_at_100: 0.0211 - ndcg_at_100: 0.0445 - map_at_100: 0.0211 - precision_at_100: 0.0015 - regularization_loss: 0.0000e+00 - loss_batch: 8.4772\n",
      "Epoch 2/5\n",
      "900/900 [==============================] - 112s 122ms/step - loss: 7.7637 - recall_at_100: 0.2720 - mrr_at_100: 0.0412 - ndcg_at_100: 0.0833 - map_at_100: 0.0412 - precision_at_100: 0.0027 - regularization_loss: 0.0000e+00 - loss_batch: 7.7633\n",
      "Epoch 3/5\n",
      "900/900 [==============================] - 112s 122ms/step - loss: 7.5142 - recall_at_100: 0.3263 - mrr_at_100: 0.0511 - ndcg_at_100: 0.1015 - map_at_100: 0.0511 - precision_at_100: 0.0033 - regularization_loss: 0.0000e+00 - loss_batch: 7.5138\n",
      "Epoch 4/5\n",
      "900/900 [==============================] - 112s 123ms/step - loss: 7.4107 - recall_at_100: 0.3475 - mrr_at_100: 0.0556 - ndcg_at_100: 0.1091 - map_at_100: 0.0556 - precision_at_100: 0.0035 - regularization_loss: 0.0000e+00 - loss_batch: 7.4103\n",
      "Epoch 5/5\n",
      "900/900 [==============================] - 113s 124ms/step - loss: 7.3449 - recall_at_100: 0.3628 - mrr_at_100: 0.0583 - ndcg_at_100: 0.1142 - map_at_100: 0.0583 - precision_at_100: 0.0036 - regularization_loss: 0.0000e+00 - loss_batch: 7.3444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8fce39ab80>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.005\n",
    ")\n",
    "model.compile(run_eagerly=False, \n",
    "              optimizer=optimizer, \n",
    "              metrics=mm.TopKMetricsAggregator.default_metrics(top_ks=[100])\n",
    "             )\n",
    "\n",
    "model.fit(loader, batch_size=BATCH_SIZE, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad2bf6e-5bfe-4831-bc07-0b1f460b313f",
   "metadata": {},
   "source": [
    "We can evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9edc817-3d25-4c4c-9429-8a36e5bdedb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step - loss: 8.1988 - recall_at_100: 0.3701 - mrr_at_100: 0.0644 - ndcg_at_100: 0.1202 - map_at_100: 0.0644 - precision_at_100: 0.0037 - regularization_loss: 0.0000e+00 - loss_batch: 8.1988\n"
     ]
    }
   ],
   "source": [
    "loader_valid =mm.Loader(valid, batch_size=BATCH_SIZE, transform=mm.ToTarget(valid.schema, \"purchase_id_first\", one_hot=True))\n",
    "\n",
    "metrics = model.evaluate(loader_valid, steps=1, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49643076-dc75-41f0-bae8-678b2fe68fbd",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1c6ea-4576-4bd4-86e7-f4972ebce380",
   "metadata": {},
   "source": [
    "In this example, we focused on concepts which are relevant for a broad range of recommender system use cases. If you compare the MRR to the competition, you will notice, that the MRR can be much higher. Following are additional techniques that can be applied to improve the MRR:\n",
    "- Data Augmentations - in the RecSys'22 challenge, we used a lot of different techniques to increase the training dataset. The techniques are specific to the dataset and we did not include it in the example:\n",
    "- Additional item features - we focused on only a few item features\n",
    "- Stacking - we stacked 17 models with a two-step approach\n",
    "- Ensemble - we ensembled 3 different stacked models\n",
    "- Hyperparameter Search - we ran multiple HPO jobs to find the best hyperparameters\n",
    "\n",
    "In addition, the MRR on the June month (test data) was in general higher than in May (validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab403bb43341787581f43b51cdd291d61392c89ddb0f92179de653921d4e05db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
