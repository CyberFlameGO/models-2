{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba6f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dcc062",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "## Iterating over Deep Learning Models using Merlin Models\n",
    "\n",
    "In this example notebook, we use the Ali-CCP: Alibaba Click and Conversion Prediction dataset to build our recommender system models. This is a dataset gathered from real-world traffic logs of the recommender system in Taobao,the largest online retail platform in the world.\n",
    "\n",
    "To download the Ali-CCP training and test datasets visit [tianchi.aliyun.com](https://tianchi.aliyun.com/dataset/dataDetail?dataId=408#1). We have curated the raw dataset via this [script]() and generated the parquet files that we will use in this example. Note that the `Ali-CCP` dataset has `click` and `conversion` target columns but we only focus on building different ranking models with binary target column `click`.\n",
    "\n",
    "We'll define several popular deep learning-based model architectures, train, and evaluate them and show how Merlin Models simplifies and eases this common and iterative process.\"\n",
    "\n",
    "### Learning objectives\n",
    "- Preparing the data with NVTabular\n",
    "- Training different deep learning-based recommender models with Merlin Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e3b9e",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c09f2",
   "metadata": {},
   "source": [
    "Let's start with importing the libraries that we'll use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b4b6efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from nvtabular.loader.tf_utils import configure_tensorflow\n",
    "configure_tensorflow()\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "from merlin.models.utils.example_utils import workflow_fit_transform, save_results\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.schema import Schema\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f2fc3",
   "metadata": {},
   "source": [
    "## Feature Engineering with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ed04a",
   "metadata": {},
   "source": [
    "When we work on a new recommender systems, we explore the dataset, first. In doing so, we define our input and output paths. We will use the parquet files in the test folder to validate our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e545253",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/workspace/data/\")\n",
    "train_path = os.path.join(DATA_FOLDER, 'train', '*.parquet')\n",
    "valid_path = os.path.join(DATA_FOLDER, 'test','*.parquet')\n",
    "output_path = '/workspace/data/processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b5ff3a",
   "metadata": {},
   "source": [
    "Our dataset has only categorical features. Below, we create continuous features using target encoding (TE) technique. Target Encoding calculates the statistics from a target variable grouped by the unique values of one or more categorical features. For example, in a binary classification problem, TE calculates the conditional probability that the target is true for each category value- a simple mean. To learn more about TE, visit this [medium blog](https://medium.com/rapids-ai/target-encoding-with-rapids-cuml-do-more-with-your-categorical-data-8c762c79e784)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152946d",
   "metadata": {},
   "source": [
    "We use a utility function, `workflow_fit_transform` perform to fit and transform steps on the raw dataset applying the operators defined in the NVTabular workflow pipeline below, and also save our workflow model. After fit and transform, the processed parquet files are saved to `output_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6db0ae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:1253: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.2 s, sys: 30.9 s, total: 54.1 s\n",
      "Wall time: 56.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "user_id = [\"user_id\"] >> Categorify(freq_threshold=5) >> TagAsUserID()\n",
    "item_id = [\"item_id\"] >> Categorify(freq_threshold=5) >> TagAsItemID()\n",
    "targets = [\"click\"] >> AddMetadata(tags=[str(Tags.BINARY_CLASSIFICATION), \"target\"])\n",
    "\n",
    "add_feat = [\"user_item_categories\", \"user_item_shops\", \"user_item_brands\", \"user_item_intentions\",\"item_category\", \"item_shop\", \"item_brand\"] >> Categorify()\n",
    "\n",
    "te_feat = (\n",
    "    [\"user_id\", \"item_id\"] + add_feat >>\n",
    "    TargetEncoding(\n",
    "        ['click'],\n",
    "        kfold=1,\n",
    "        p_smooth=20\n",
    "    ) >>\n",
    "    Normalize()\n",
    ")\n",
    "\n",
    "outputs = user_id + item_id + targets + add_feat + te_feat\n",
    "\n",
    "# Remove rows where item_id==0 and user_id==0\n",
    "outputs = outputs>> Filter(f=lambda df: (df[\"item_id\"] != 0) & (df[\"user_id\"] != 0))\n",
    "\n",
    "workflow_fit_transform(outputs, train_path, valid_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7159aa53",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Recommender Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f61fa",
   "metadata": {},
   "source": [
    "NVTabular exported the schema file of our processed dataset. The `schema.pbtxt` is a protobuf text file contains features metadata, including statistics about features such as cardinality, min and max values and also tags based on their characteristics and dtypes (e.g., categorical, continuous, list, item_id). The metadata information is loaded from schema and their tags are used to automatically set the parameters of Merlin Models. In other words, Merlin Models relies on the schema object to automatically build all necessary input and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5298796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/core/dataframe.py:1253: UserWarning: The deep parameter is ignored and is only included for pandas compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train = Dataset(os.path.join(output_path, 'train', '*.parquet'), part_size=\"500MB\")\n",
    "valid = Dataset(os.path.join(output_path, 'valid', '*.parquet'), part_size=\"500MB\")\n",
    "\n",
    "# define schema object\n",
    "schema = train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f21cba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'click'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = schema.select_by_tag(Tags.TARGET).column_names[0]\n",
    "target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e13ebf",
   "metadata": {},
   "source": [
    "We can print out all the features that are included in the `schema.pbtxt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c1056d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'item_id',\n",
       " 'click',\n",
       " 'user_item_categories',\n",
       " 'user_item_shops',\n",
       " 'user_item_brands',\n",
       " 'user_item_intentions',\n",
       " 'item_category',\n",
       " 'item_shop',\n",
       " 'item_brand',\n",
       " 'TE_user_item_categories_click',\n",
       " 'TE_user_item_shops_click',\n",
       " 'TE_user_item_brands_click',\n",
       " 'TE_user_item_intentions_click',\n",
       " 'TE_item_category_click',\n",
       " 'TE_item_shop_click',\n",
       " 'TE_item_brand_click',\n",
       " 'TE_user_id_click',\n",
       " 'TE_item_id_click']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf60006",
   "metadata": {},
   "source": [
    "### Initialize Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701c9d2",
   "metadata": {},
   "source": [
    "We're ready to start training, for that, we create our dataset objects, and under the hood we use Merlin `BatchedDataset` class for reading chunks of parquet files. `BatchedDataset` asynchronously iterate through CSV or Parquet dataframes on GPU by leveraging an NVTabular `Dataset`. To read more about Merlin optimized dataloaders visit [here](https://github.com/NVIDIA-Merlin/models/blob/main/merlin/models/tf/dataset.py#L141)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58aa718",
   "metadata": {},
   "source": [
    "### NCF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0193b9",
   "metadata": {},
   "source": [
    "We will first build and train a Neural Collaborative Filtering (NCF) model. Neural Collaborative Filtering [(NCF)](https://arxiv.org/pdf/1708.05031.pdf) Model  architecture explores neural network architectures for collaborative filtering, in other words explores the use of deep neural networks for learning the interaction function from data.\n",
    "\n",
    "NCF feed categorical features into embedding layer, concat the embedding outputs and add multiple hidden layers via its MLP layer tower as seen in the figure. GMF and MLP uses separate user and item embeddings, and then outputs of their interactions from GMF Layer and MLP Layer are concatenated and fed to the final NeuMF (Neural Matrix Factorisation) layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e65237",
   "metadata": {},
   "source": [
    "<img src=\"./images/ncf.png\"  width=\"30%\">\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/1708.05031.pdf\">Image Source: NCF paper</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d72e778",
   "metadata": {},
   "source": [
    "With `schema` object we enable NCF model easily to recognize item_id and user_id columns (defined in the schema.pbtxt with corresponding tags). Input block of embedding layers will be generated using item_id and user_id as seen in the Figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "446a597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.benchmark.NCFModel(\n",
    "    schema,\n",
    "    embedding_dim=64,\n",
    "    mlp_block=mm.MLPBlock([128, 64]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column, metrics=[tf.keras.metrics.AUC()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1550cafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 20:30:08.882215: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2207/2209 [============================>.] - ETA: 0s - auc: 0.5288 - loss: 0.1822 - regularization_loss: 0.0000e+00 - total_loss: 0.1822"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 20:31:15.550433: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: cond/else/_1/cond/cond/branch_executed/_139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2209/2209 [==============================] - 90s 34ms/step - auc: 0.5288 - loss: 0.1822 - regularization_loss: 0.0000e+00 - total_loss: 0.1822 - val_auc: 0.5001 - val_loss: 0.1332 - val_regularization_loss: 0.0000e+00 - val_total_loss: 0.1332\n",
      "CPU times: user 2min 20s, sys: 21.4 s, total: 2min 41s\n",
      "Wall time: 1min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6e147f88e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 16*1024\n",
    "LR=0.01\n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=LR)\n",
    "model.compile(optimizer=opt, run_eagerly=False)\n",
    "model.fit(train, validation_data=valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c203cea",
   "metadata": {},
   "source": [
    "Let's save our accuracy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ca070aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results('NCF', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d865fe",
   "metadata": {},
   "source": [
    "### MLP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e117b97",
   "metadata": {},
   "source": [
    "Now we will change our model to Multi-Layer Percepton (MLP) model. MLP models feed categorical features into embedding layer, concat the embedding outputs and add multiple hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653f236",
   "metadata": {},
   "source": [
    "<img src=\"./images/mlp.png\"  width=\"30%\">\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Change the model to MLP model\n",
    "- Rerun the pipeline from there from model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2245f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses default embedding_dim = 64\n",
    "model = mm.MLPBlock([64, 32]).to_model(\n",
    "    schema, \n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column, metrics=[tf.keras.metrics.AUC()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a3cdfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206/2209 [============================>.] - ETA: 0s - auc_1: 0.6727 - loss: 0.1657 - regularization_loss: 0.0000e+00 - total_loss: 0.1657"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 20:32:40.620134: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: cond/then/_0/cond/cond/branch_executed/_140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2209/2209 [==============================] - 88s 38ms/step - auc_1: 0.6727 - loss: 0.1656 - regularization_loss: 0.0000e+00 - total_loss: 0.1656 - val_auc_1: 0.5735 - val_loss: 0.1362 - val_regularization_loss: 0.0000e+00 - val_total_loss: 0.1362\n",
      "CPU times: user 2min 50s, sys: 28.3 s, total: 3min 18s\n",
      "Wall time: 1min 29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6e14471e20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=LR)\n",
    "model.compile(optimizer=opt, run_eagerly=False)\n",
    "model.fit(train, validation_data=valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21f6befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results('MLP', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83126e5e",
   "metadata": {},
   "source": [
    "### DLRM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d6171",
   "metadata": {},
   "source": [
    "Deep Learning Recommendation Model [(DLRM)](https://arxiv.org/abs/1906.00091) architecture is a popular neural network model originally proposed by Facebook in 2019 as a personalization deep learning model.\n",
    "\n",
    "![DLRM](./images/DLRM.png)\n",
    "\n",
    "\n",
    "DLRM accepts two types of features: categorical and numerical. \n",
    "- For each categorical feature, an embedding table is used to provide dense representation to each unique value. \n",
    "- For numerical features, they are fed to model as dense features, and then transformed by a simple neural network referred to as \"bottom MLP\". This part of the network consists of a series of linear layers with ReLU activations. \n",
    "- The output of the bottom MLP and the embedding vectors are then fed into the dot product interaction operation (see Pairwise interaction step). The output of \"dot interaction\" is then concatenated with the features resulting from the bottom MLP (we apply a skip-connection there) and fed into the \"top MLP\" which is also a series of dense layers with activations ((a fully connected NN). \n",
    "- The model outputs a single number (here we use sigmoid function to generate probabilities) which can be interpreted as a likelihood of a certain user clicking on an ad, watching a movie, or viewing a news page.\n",
    "\n",
    "\n",
    "Steps:<br>\n",
    "* Change the model to `DLRMModel`\n",
    "* Rerun the pipeline from there from model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47c4f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DLRMModel(\n",
    "    schema,\n",
    "    embedding_dim=64,\n",
    "    bottom_block=mm.MLPBlock([128, 64]),\n",
    "    top_block=mm.MLPBlock([128, 64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column, metrics=[tf.keras.metrics.AUC()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a58a75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6/2209 [..............................] - ETA: 1:44 - auc_2: 0.5748 - loss: 0.6834 - regularization_loss: 0.0000e+00 - total_loss: 0.6834WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0215s vs `on_train_batch_end` time: 0.0218s). Check your callbacks.\n",
      "2207/2209 [============================>.] - ETA: 0s - auc_2: 0.6962 - loss: 0.1608 - regularization_loss: 0.0000e+00 - total_loss: 0.1608"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 20:34:37.089910: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: cond/then/_0/cond/cond/branch_executed/_158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2209/2209 [==============================] - 117s 49ms/step - auc_2: 0.6962 - loss: 0.1608 - regularization_loss: 0.0000e+00 - total_loss: 0.1608 - val_auc_2: 0.5722 - val_loss: 0.1366 - val_regularization_loss: 0.0000e+00 - val_total_loss: 0.1366\n",
      "CPU times: user 3min 37s, sys: 37.8 s, total: 4min 15s\n",
      "Wall time: 1min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f69977a8190>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=LR)\n",
    "model.compile(optimizer=opt, run_eagerly=False)\n",
    "model.fit(train, validation_data=valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f330c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(\"DLRM\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ffe561",
   "metadata": {},
   "source": [
    "### DCN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245534c4",
   "metadata": {},
   "source": [
    "DCN-V2 is an architecture proposed as an improvement upon the original [DCN model](https://arxiv.org/pdf/1708.05123.pdf). The explicit feature interactions of the inputs are learned through cross layers, and then combined with a deep network to learn complementary implicit interactions. The overall model architecture is depicted in Figure below, with two ways to combine the cross network with the deep network: (1) stacked and (2) parallel. The output of the embbedding layer is the concatenation of all the embedded vectors and the normalized dense features: x<sub>0</sub> = [x<sub>embed,1</sub>; . . . ; x<sub>embed,𝑛</sub>; 𝑥<sub>dense</sub>].\n",
    "\n",
    "![DCN](./images/DCN.png)\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/2008.13535\">Image Source: DCN V2 paper</a>\n",
    "\n",
    "In this example, we build `DCN-v2 stacked` architecture. \n",
    "\n",
    "Steps:<br>\n",
    "* Change the model to `DCNModel`\n",
    "* Rerun the pipeline from there to model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58b1f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.DCNModel(\n",
    "    schema,\n",
    "    depth=2,\n",
    "    deep_block=mm.MLPBlock([64, 32]),\n",
    "    prediction_tasks=mm.BinaryClassificationTask(target_column, metrics=[tf.keras.metrics.AUC()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39fa5814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2208/2209 [============================>.] - ETA: 0s - auc_3: 0.6513 - loss: 0.1715 - regularization_loss: 0.0000e+00 - total_loss: 0.1715"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 20:36:22.316188: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:907] Skipping loop optimization for Merge node with control input: cond/then/_0/cond/cond/branch_executed/_152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2209/2209 [==============================] - 104s 44ms/step - auc_3: 0.6513 - loss: 0.1715 - regularization_loss: 0.0000e+00 - total_loss: 0.1715 - val_auc_3: 0.5723 - val_loss: 0.1369 - val_regularization_loss: 0.0000e+00 - val_total_loss: 0.1369\n",
      "CPU times: user 3min 20s, sys: 36.6 s, total: 3min 57s\n",
      "Wall time: 1min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6996679700>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=0.005)\n",
    "model.compile(optimizer=opt, run_eagerly=False)\n",
    "model.fit(train, validation_data=valid, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c1e6e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(\"DCN\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bab3f44",
   "metadata": {},
   "source": [
    "Let's visualize our model validation accuracy values. Since we did not do any hyper-parameter optimization or extensive feature engineering here, we do not come up with a final conclusion that one model is superior to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6477faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def create_bar_chart(text_file_name, models_name):\n",
    "    \"\"\"a func to plot barcharts via parsing the  accurracy results in a text file\"\"\"\n",
    "    auc = []\n",
    "    with open(text_file_name, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            if \"auc\" in line:\n",
    "                data = [line.rstrip().split(\":\")]\n",
    "                key, value = zip(*data)\n",
    "                auc.append(float(value[0]))\n",
    "\n",
    "    X_axis = np.arange(len(models_name))\n",
    "\n",
    "    plt.title(\"Models' accuracy metrics comparison\", pad=20)\n",
    "    plt.bar(X_axis - 0.2, auc, 0.4, label=\"AUC\")\n",
    "\n",
    "    plt.xticks(X_axis, models_name)\n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f537685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models_name = [\"NCF\", \"MLP\", \"DLRM\", \"DCN\"]\n",
    "create_bar_chart(\"results.txt\", models_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
